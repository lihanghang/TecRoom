<!DOCTYPE html>
<html data-content_root="../../" lang="zh-CN">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <meta content="width=device-width, initial-scale=1" name="viewport"/>
  <meta content="width=device-width,initial-scale=1" name="viewport"/>
  <meta content="ie=edge" http-equiv="x-ua-compatible"/>
  <meta content="Copy to clipboard" name="lang:clipboard.copy"/>
  <meta content="Copied to clipboard" name="lang:clipboard.copied"/>
  <meta content="en" name="lang:search.language"/>
  <meta content="True" name="lang:search.pipeline.stopwords"/>
  <meta content="True" name="lang:search.pipeline.trimmer"/>
  <meta content="No matching documents" name="lang:search.result.none"/>
  <meta content="1 matching document" name="lang:search.result.one"/>
  <meta content="# matching documents" name="lang:search.result.other"/>
  <meta content="[\s\-]+" name="lang:search.tokenizer"/>
  <link crossorigin="" href="https://fonts.gstatic.com/" rel="preconnect"/>
  <link href="https://fonts.googleapis.com/css?family=Roboto+Mono:400,500,700|Roboto:300,400,400i,700&amp;display=fallback" rel="stylesheet"/>
  <style>
   body,
      input {
        font-family: "Roboto", "Helvetica Neue", Helvetica, Arial, sans-serif
      }

      code,
      kbd,
      pre {
        font-family: "Roboto Mono", "Courier New", Courier, monospace
      }
  </style>
  <link href="../../_static/stylesheets/application.css" rel="stylesheet"/>
  <link href="../../_static/stylesheets/application-palette.css" rel="stylesheet"/>
  <link href="../../_static/stylesheets/application-fixes.css" rel="stylesheet"/>
  <link href="../../_static/fonts/material-icons.css" rel="stylesheet"/>
  <meta content="#2196f3" name="theme-color"/>
  <script src="../../_static/javascripts/modernizr.js">
  </script>
  <link href="../../_static/images/apple-icon-152x152.png" rel="apple-touch-icon"/>
  <title>
   4. 自然语言理解 — Hang's Tec Room 2.1.0 文档
  </title>
  <link href="../../_static/pygments.css?v=03e43079" rel="stylesheet" type="text/css"/>
  <link href="../../_static/material.css?v=79c92029" rel="stylesheet" type="text/css"/>
  <link href="../../_static/graphviz.css?v=4ae1632d" rel="stylesheet" type="text/css"/>
  <link href="../../_static/copybutton.css?v=76b2166b" rel="stylesheet" type="text/css"/>
  <link href="../../_static/togglebutton.css?v=13237357" rel="stylesheet" type="text/css"/>
  <link href="../../_static/sphinx-design.min.css?v=fb60a719" rel="stylesheet" type="text/css"/>
  <script src="../../_static/documentation_options.js?v=2e1fee42">
  </script>
  <script src="../../_static/doctools.js?v=9bcbadda">
  </script>
  <script src="../../_static/sphinx_highlight.js?v=dc90522c">
  </script>
  <script src="../../_static/clipboard.min.js?v=a7894cd8">
  </script>
  <script src="../../_static/copybutton.js?v=f281be69">
  </script>
  <script>
   let toggleHintShow = 'Click to show';
  </script>
  <script>
   let toggleHintHide = 'Click to hide';
  </script>
  <script>
   let toggleOpenOnPrint = 'true';
  </script>
  <script src="../../_static/togglebutton.js?v=4a39c7ea">
  </script>
  <script src="../../_static/translations.js?v=beaddf03">
  </script>
  <script src="../../_static/design-tabs.js?v=f930bc37">
  </script>
  <script>
   var togglebuttonSelector = '.toggle, .admonition.dropdown';
  </script>
  <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
  <link href="../../genindex.html" rel="index" title="索引"/>
  <link href="../../search.html" rel="search" title="搜索"/>
  <link href="kg.html" rel="next" title="6. 知识图谱(Knowledge Graph)"/>
  <link href="dl.html" rel="prev" title="3. 深度学习"/>
  <link href="../../_static/images/apple-icon-152x152.png" rel="apple-touch-icon"/>
 </head>
 <body data-md-color-accent="cyan" data-md-color-primary="blue" dir="ltr">
  <svg class="md-svg">
   <defs data-children-count="0">
    <svg height="448" id="__github" viewbox="0 0 416 448" width="416" xmlns="http://www.w3.org/2000/svg">
     <path d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z" fill="currentColor">
     </path>
    </svg>
   </defs>
  </svg>
  <input class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/>
  <input class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/>
  <label class="md-overlay" data-md-component="overlay" for="__drawer">
  </label>
  <a class="md-skip" href="#doc/机器智能系列/nlp" tabindex="1">
   Skip to content
  </a>
  <header class="md-header" data-md-component="header">
   <nav class="md-header-nav md-grid">
    <div class="md-flex navheader">
     <div class="md-flex__cell md-flex__cell--shrink">
      <a class="md-header-nav__button md-logo" href="../../index.html" title="Hang's Tec Room 2.1.0 文档">
       <i class="md-icon">
        
       </i>
      </a>
     </div>
     <div class="md-flex__cell md-flex__cell--shrink">
      <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer">
      </label>
     </div>
     <div class="md-flex__cell md-flex__cell--stretch">
      <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
       <span class="md-header-nav__topic">
        Hang's Tec Room 2.1.0 文档
       </span>
       <span class="md-header-nav__topic">
        4. 自然语言理解
       </span>
      </div>
     </div>
     <div class="md-flex__cell md-flex__cell--shrink">
      <label class="md-icon md-icon--search md-header-nav__button" for="__search">
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
       <label class="md-search__overlay" for="__search">
       </label>
       <div class="md-search__inner" role="search">
        <form action="../../search.html" class="md-search__form" method="get" name="search">
         <input autocapitalize="off" autocomplete="off" class="md-search__input" data-md-component="query" data-md-state="active" name="q" placeholder="" search""="" spellcheck="false" type="text"/>
         <label class="md-icon md-search__icon" for="__search">
         </label>
         <button class="md-icon md-search__icon" data-md-component="reset" tabindex="-1" type="reset">
          
         </button>
        </form>
        <div class="md-search__output">
         <div class="md-search__scrollwrap" data-md-scrollfix="">
          <div class="md-search-result" data-md-component="result">
           <div class="md-search-result__meta">
            Type to start searching
           </div>
           <ol class="md-search-result__list">
           </ol>
          </div>
         </div>
        </div>
       </div>
      </div>
     </div>
     <div class="md-flex__cell md-flex__cell--shrink">
      <div class="md-header-nav__source">
       <a class="md-source" data-md-source="github" href="https://github.com/lihanghang/TecRoom" title="Go to repository">
        <div class="md-source__icon">
         <svg height="28" viewbox="0 0 24 24" width="28" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
          <use height="24" width="24" xlink:href="#__github">
          </use>
         </svg>
        </div>
        <div class="md-source__repository">
         TecRoom
        </div>
       </a>
      </div>
     </div>
     <script src="../../_static/javascripts/version_dropdown.js">
     </script>
     <script>
      var json_loc = "../../"versions.json"",
        target_loc = "../../../",
        text = "Versions";
    $( document ).ready( add_version_dropdown(json_loc, target_loc, text));
     </script>
    </div>
   </nav>
  </header>
  <div class="md-container">
   <nav class="md-tabs" data-md-component="tabs">
    <div class="md-tabs__inner md-grid">
     <ul class="md-tabs__list">
      <li class="md-tabs__item">
       <a class="md-tabs__link" href="../../index.html">
        主页
       </a>
      </li>
      <li class="md-tabs__item">
       <a class="md-tabs__link" href="https://lihanghang.top">
        博客
       </a>
      </li>
     </ul>
    </div>
   </nav>
   <main class="md-main">
    <div class="md-main__inner md-grid" data-md-component="container">
     <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
      <div class="md-sidebar__scrollwrap">
       <div class="md-sidebar__inner">
        <nav class="md-nav md-nav--primary" data-md-level="0">
         <label class="md-nav__title md-nav__title--site" for="__drawer">
          <a class="md-nav__button md-logo" href="../../index.html" title="Hang's Tec Room 2.1.0 文档">
           <i class="md-icon">
            
           </i>
          </a>
          <a href="../../index.html" title="Hang's Tec Room 2.1.0 文档">
           Hang's Tec Room 2.1.0 文档
          </a>
         </label>
         <div class="md-nav__source">
          <a class="md-source" data-md-source="github" href="https://github.com/lihanghang/TecRoom" title="Go to repository">
           <div class="md-source__icon">
            <svg height="28" viewbox="0 0 24 24" width="28" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
             <use height="24" width="24" xlink:href="#__github">
             </use>
            </svg>
           </div>
           <div class="md-source__repository">
            TecRoom
           </div>
          </a>
         </div>
         <ul class="md-nav__list">
          <li class="md-nav__item">
           <span class="md-nav__link caption">
            <span class="caption-text">
             关于
            </span>
           </span>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../%E5%85%B3%E4%BA%8E/index.html">
            公共资源
           </a>
           <ul class="md-nav__list">
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E5%85%B3%E4%BA%8E/about_me.html">
              1. 关于我
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E5%85%B3%E4%BA%8E/about_blog.html">
              2. 关于技术屋
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E5%85%B3%E4%BA%8E/build_readthedocs.html">
              3. 基于ReadtheDocs托管在线知识库的步骤
             </a>
            </li>
           </ul>
          </li>
          <li class="md-nav__item">
           <span class="md-nav__link caption">
            <span class="caption-text">
             编程语言及开发框架
            </span>
           </span>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%8F%8A%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/index.html">
            语言及框架
           </a>
           <ul class="md-nav__list">
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%8F%8A%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/C%2B%2B.html">
              1. C++
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%8F%8A%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/Java%E6%8A%80%E6%9C%AF%E6%A0%88/Java%E5%9F%BA%E7%A1%80%E6%8A%80%E6%9C%AF%E4%BD%93%E7%B3%BB.html">
              2. Java基础技术体系
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%8F%8A%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/Java%E6%8A%80%E6%9C%AF%E6%A0%88/%E5%B8%B8%E7%94%A8%E6%A1%86%E6%9E%B6/Spring.html">
              3. Spring
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%8F%8A%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/Java%E6%8A%80%E6%9C%AF%E6%A0%88/%E5%B8%B8%E7%94%A8%E6%A1%86%E6%9E%B6/SpringBoot.html">
              4. SpringBoot基础
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%8F%8A%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/Java%E6%8A%80%E6%9C%AF%E6%A0%88/%E5%B8%B8%E7%94%A8%E6%A1%86%E6%9E%B6/SpringBoot.html#id6">
              5. SpringBoot进阶
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%8F%8A%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/python%E6%8A%80%E6%9C%AF%E6%A0%88/Python.html">
              6. Python基础知识
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%8F%8A%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/python%E6%8A%80%E6%9C%AF%E6%A0%88/%E5%B8%B8%E7%94%A8%E6%A1%86%E6%9E%B6/django.html">
              7. Django
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%8F%8A%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/Drools.html">
              8. Drools规则引擎
             </a>
            </li>
           </ul>
          </li>
          <li class="md-nav__item">
           <span class="md-nav__link caption">
            <span class="caption-text">
             后端接口开发
            </span>
           </span>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../%E6%8E%A5%E5%8F%A3%E5%BC%80%E5%8F%91/%E6%8E%A5%E5%8F%A3%E5%BC%80%E5%8F%91.html">
            1. 接口（Interface）开发
           </a>
           <ul class="md-nav__list">
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E6%8E%A5%E5%8F%A3%E5%BC%80%E5%8F%91/%E6%8E%A5%E5%8F%A3%E5%BC%80%E5%8F%91.html#id1">
              1.1. 参考
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E6%8E%A5%E5%8F%A3%E5%BC%80%E5%8F%91/%E6%8E%A5%E5%8F%A3%E5%BC%80%E5%8F%91.html#id2">
              1.2. 几个概念
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E6%8E%A5%E5%8F%A3%E5%BC%80%E5%8F%91/%E6%8E%A5%E5%8F%A3%E5%BC%80%E5%8F%91.html#id4">
              1.3. 接口的好处
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E6%8E%A5%E5%8F%A3%E5%BC%80%E5%8F%91/%E6%8E%A5%E5%8F%A3%E5%BC%80%E5%8F%91.html#id5">
              1.4. 接口定义原则及规范
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E6%8E%A5%E5%8F%A3%E5%BC%80%E5%8F%91/%E6%8E%A5%E5%8F%A3%E5%BC%80%E5%8F%91.html#id11">
              1.5. 项目结构
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E6%8E%A5%E5%8F%A3%E5%BC%80%E5%8F%91/%E6%8E%A5%E5%8F%A3%E5%BC%80%E5%8F%91.html#id12">
              1.6. 接口开发经验
             </a>
            </li>
           </ul>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../%E6%8E%A5%E5%8F%A3%E5%BC%80%E5%8F%91/%E6%8E%A5%E5%8F%A3%E6%A1%86%E6%9E%B6.html">
            2. 常见接口框架
           </a>
           <ul class="md-nav__list">
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E6%8E%A5%E5%8F%A3%E5%BC%80%E5%8F%91/%E6%8E%A5%E5%8F%A3%E6%A1%86%E6%9E%B6.html#flask">
              2.1. Flask
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E6%8E%A5%E5%8F%A3%E5%BC%80%E5%8F%91/%E6%8E%A5%E5%8F%A3%E6%A1%86%E6%9E%B6.html#fastapi">
              2.2. FastAPI
             </a>
            </li>
           </ul>
          </li>
          <li class="md-nav__item">
           <span class="md-nav__link caption">
            <span class="caption-text">
             项目开发杂谈
            </span>
           </span>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E6%9D%82%E8%B0%88/%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E6%9D%82%E8%B0%88.html">
            1. 项目开发杂谈
           </a>
           <ul class="md-nav__list">
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E6%9D%82%E8%B0%88/%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E6%9D%82%E8%B0%88.html#id2">
              1.1. 软件迭代的环境问题
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E6%9D%82%E8%B0%88/%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E6%9D%82%E8%B0%88.html#id3">
              1.2. 软件开发的流程
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E6%9D%82%E8%B0%88/%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E6%9D%82%E8%B0%88.html#id4">
              1.3. 工程结构问题
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E6%9D%82%E8%B0%88/%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E6%9D%82%E8%B0%88.html#id5">
              1.4. 本地开发环境
             </a>
            </li>
           </ul>
          </li>
          <li class="md-nav__item">
           <span class="md-nav__link caption">
            <span class="caption-text">
             数据结构与算法系列
            </span>
           </span>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97/dataStruct_algorithm.html">
            1. 数据结构(Data Structure)
           </a>
           <ul class="md-nav__list">
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97/dataStruct_algorithm.html#id1">
              1.1. 方法论
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97/dataStruct_algorithm.html#id3">
              1.2. 数据结构
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97/dataStruct_algorithm.html#array">
              1.3. Array
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97/dataStruct_algorithm.html#stack-and-queue">
              1.4. Stack and Queue
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97/dataStruct_algorithm.html#priorityqueue">
              1.5. PriorityQueue
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97/dataStruct_algorithm.html#linkedlist-single-double">
              1.6. LinkedList(single/double)
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97/dataStruct_algorithm.html#tree-binary-tree">
              1.7. Tree/Binary Tree
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97/dataStruct_algorithm.html#hashtable">
              1.8. HashTable
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97/dataStruct_algorithm.html#disjoint-set">
              1.9. Disjoint Set
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97/dataStruct_algorithm.html#trie">
              1.10. Trie
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97/dataStruct_algorithm.html#bloomfilter">
              1.11. BloomFilter
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97/dataStruct_algorithm.html#lru-cache">
              1.12. LRU Cache
             </a>
            </li>
           </ul>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97/dataStruct_algorithm.html#algorithm">
            2. 算法（Algorithm)
           </a>
           <ul class="md-nav__list">
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97/dataStruct_algorithm.html#general-coding">
              2.1. General Coding
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97/dataStruct_algorithm.html#in-order-pre-order-post-order-traversal">
              2.2. In-order/pre-order/Post-order traversal
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97/dataStruct_algorithm.html#greedy">
              2.3. Greedy
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97/dataStruct_algorithm.html#recursion-backtrace">
              2.4. Recursion/Backtrace
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97/dataStruct_algorithm.html#breadth-first-search">
              2.5. Breadth-first search
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97/dataStruct_algorithm.html#depth-first-search">
              2.6. Depth-first search
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97/dataStruct_algorithm.html#divide-and-conquer">
              2.7. Divide and Conquer
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97/dataStruct_algorithm.html#dynamic-programming">
              2.8. Dynamic Programming
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97/dataStruct_algorithm.html#binary-search">
              2.9. Binary search
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97/dataStruct_algorithm.html#graph">
              2.10. Graph
             </a>
            </li>
           </ul>
          </li>
          <li class="md-nav__item">
           <span class="md-nav__link caption">
            <span class="caption-text">
             设计模式
            </span>
           </span>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/design-pattern.html">
            1. 设计模式
           </a>
           <ul class="md-nav__list">
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/design-pattern.html#id2">
              1.1. 资源推荐
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/design-pattern.html#id4">
              1.2. 知识框架图
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/design-pattern.html#id5">
              1.3. 初识设计模式
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/design-pattern.html#id7">
              1.4. 创建型（5）
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/design-pattern.html#id13">
              1.5. 结构型（7）
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/design-pattern.html#id21">
              1.6. 行为型（11）
             </a>
            </li>
           </ul>
          </li>
          <li class="md-nav__item">
           <span class="md-nav__link caption">
            <span class="caption-text">
             计算机网络技术
            </span>
           </span>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E6%8A%80%E6%9C%AF/computer-network.html">
            1. 计算机网络系列
           </a>
          </li>
          <li class="md-nav__item">
           <span class="md-nav__link caption">
            <span class="caption-text">
             数据库系列
            </span>
           </span>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E5%88%97/mysql.html">
            1. 关系数据库-MySQL
           </a>
           <ul class="md-nav__list">
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E5%88%97/mysql.html#id1">
              1.1. 书籍推荐
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E5%88%97/mysql.html#id2">
              1.2. 基础知识
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E5%88%97/mysql.html#id4">
              1.3. 设计规约
             </a>
            </li>
           </ul>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E5%88%97/mysql.html#redis">
            2. 非关系数据库--Redis
           </a>
          </li>
          <li class="md-nav__item">
           <span class="md-nav__link caption">
            <span class="caption-text">
             虚拟化与容器技术
            </span>
           </span>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../%E8%99%9A%E6%8B%9F%E5%8C%96%E4%B8%8E%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/docker.html">
            1. Docker
           </a>
           <ul class="md-nav__list">
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E8%99%9A%E6%8B%9F%E5%8C%96%E4%B8%8E%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/docker.html#id1">
              1.1. 书籍推荐
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E8%99%9A%E6%8B%9F%E5%8C%96%E4%B8%8E%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/docker.html#id4">
              1.2. 初识Docker
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E8%99%9A%E6%8B%9F%E5%8C%96%E4%B8%8E%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/docker.html#id5">
              1.3. 图述Docker
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E8%99%9A%E6%8B%9F%E5%8C%96%E4%B8%8E%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/docker.html#id6">
              1.4. Docker架构（组件）
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E8%99%9A%E6%8B%9F%E5%8C%96%E4%B8%8E%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/docker.html#docker-mac">
              1.5. 安装Docker（MAC)并注册国内镜像加速器
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E8%99%9A%E6%8B%9F%E5%8C%96%E4%B8%8E%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/docker.html#id14">
              1.6. Docker常用命令
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E8%99%9A%E6%8B%9F%E5%8C%96%E4%B8%8E%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/docker.html#id16">
              1.7. Docker容器的数据卷
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E8%99%9A%E6%8B%9F%E5%8C%96%E4%B8%8E%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/docker.html#id22">
              1.8. Docker使用案例（应用部署实战）
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E8%99%9A%E6%8B%9F%E5%8C%96%E4%B8%8E%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/docker.html#dockerfile">
              1.9. 使用Dockerfile制作镜像
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E8%99%9A%E6%8B%9F%E5%8C%96%E4%B8%8E%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/docker.html#docker-compose">
              1.10. docker-compose（服务编排技术）
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E8%99%9A%E6%8B%9F%E5%8C%96%E4%B8%8E%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/docker.html#id29">
              1.11. 搭建私有仓库
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E8%99%9A%E6%8B%9F%E5%8C%96%E4%B8%8E%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/docker.html#id32">
              1.12. 结束
             </a>
            </li>
           </ul>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../%E8%99%9A%E6%8B%9F%E5%8C%96%E4%B8%8E%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/k8s.html">
            2. Kubernetes（K8S）
           </a>
           <ul class="md-nav__list">
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E8%99%9A%E6%8B%9F%E5%8C%96%E4%B8%8E%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/k8s.html#id1">
              2.1. 推荐
             </a>
            </li>
           </ul>
          </li>
          <li class="md-nav__item">
           <span class="md-nav__link caption">
            <span class="caption-text">
             服务器
            </span>
           </span>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../%E6%9C%8D%E5%8A%A1%E5%99%A8/server.html">
            1. 服务器系列
           </a>
           <ul class="md-nav__list">
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E6%9C%8D%E5%8A%A1%E5%99%A8/server.html#linux">
              1.1. Linux 基础知识
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E6%9C%8D%E5%8A%A1%E5%99%A8/server.html#id2">
              1.2. Linux 常用命令
             </a>
            </li>
           </ul>
          </li>
          <li class="md-nav__item">
           <span class="md-nav__link caption">
            <span class="caption-text">
             分布式与微服务系列
            </span>
           </span>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%8E%E5%BE%AE%E6%9C%8D%E5%8A%A1%E7%B3%BB%E5%88%97/index.html">
            分布式与微服务
           </a>
           <ul class="md-nav__list">
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%8E%E5%BE%AE%E6%9C%8D%E5%8A%A1%E7%B3%BB%E5%88%97/%E4%B8%AD%E9%97%B4%E4%BB%B6.html">
              1. 中间件集合
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%8E%E5%BE%AE%E6%9C%8D%E5%8A%A1%E7%B3%BB%E5%88%97/distributeMicroServer.html">
              2. 分布式与微服务系列
             </a>
            </li>
           </ul>
          </li>
          <li class="md-nav__item">
           <span class="md-nav__link caption">
            <span class="caption-text">
             机器智能系列
            </span>
           </span>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="ml.html">
            1. 机器学习基础
           </a>
           <ul class="md-nav__list">
            <li class="md-nav__item">
             <a class="md-nav__link" href="ml.html#id2">
              1.1. 书籍推荐
             </a>
            </li>
           </ul>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="ml.html#id3">
            2. 机器学习应用
           </a>
           <ul class="md-nav__list">
            <li class="md-nav__item">
             <a class="md-nav__link" href="ml.html#id4">
              2.1. 常见应用领域
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="ml.html#id5">
              2.2. 常见算法回顾
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="ml.html#id6">
              2.3. 模型评价指标
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="ml.html#id7">
              2.4. 常用建模工具
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="ml.html#id8">
              2.5. 实际问题建模思路流程
             </a>
            </li>
           </ul>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="dl.html">
            3. 深度学习
           </a>
           <ul class="md-nav__list">
            <li class="md-nav__item">
             <a class="md-nav__link" href="dl.html#id2">
              3.1. 书籍推荐
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="dl.html#id3">
              3.2. 深度学习基本点
             </a>
            </li>
           </ul>
          </li>
          <li class="md-nav__item">
           <input class="md-toggle md-nav__toggle" data-md-toggle="toc" id="__toc" type="checkbox"/>
           <label class="md-nav__link md-nav__link--active" for="__toc">
            4. 自然语言理解
           </label>
           <a class="md-nav__link md-nav__link--active" href="#">
            4. 自然语言理解
           </a>
           <nav class="md-nav md-nav--secondary">
            <label class="md-nav__title" for="__toc">
             "Contents"
            </label>
            <ul class="md-nav__list" data-md-scrollfix="">
             <li class="md-nav__item">
              <a class="md-nav__link" href="#doc-ji-qi-zhi-neng-xi-lie-nlp--page-root">
               4. 自然语言理解
              </a>
              <nav class="md-nav">
               <ul class="md-nav__list">
                <li class="md-nav__item">
                 <a class="md-nav__link" href="#id2">
                  4.1. 书籍推荐
                 </a>
                </li>
                <li class="md-nav__item">
                 <a class="md-nav__link" href="#id3">
                  4.2. 什么是 自然语言 理解？
                 </a>
                 <nav class="md-nav">
                  <ul class="md-nav__list">
                   <li class="md-nav__item">
                    <a class="md-nav__link" href="#id4">
                     4.2.1. 自然语言？
                    </a>
                   </li>
                   <li class="md-nav__item">
                    <a class="md-nav__link" href="#id5">
                     4.2.2. 自然语言理解？
                    </a>
                   </li>
                   <li class="md-nav__item">
                    <a class="md-nav__link" href="#id6">
                     4.2.3. 自然语言的特点
                    </a>
                   </li>
                   <li class="md-nav__item">
                    <a class="md-nav__link" href="#id7">
                     4.2.4. 难点
                    </a>
                   </li>
                   <li class="md-nav__item">
                    <a class="md-nav__link" href="#id8">
                     4.2.5. 未来
                    </a>
                   </li>
                   <li class="md-nav__item">
                    <a class="md-nav__link" href="#id9">
                     4.2.6. 自然语言理解交叉科学
                    </a>
                   </li>
                  </ul>
                 </nav>
                </li>
                <li class="md-nav__item">
                 <a class="md-nav__link" href="#id10">
                  4.3. 自然语言技术方向
                 </a>
                </li>
                <li class="md-nav__item">
                 <a class="md-nav__link" href="#nlp">
                  4.4. NLP国内外优秀学者及实验室
                 </a>
                </li>
               </ul>
              </nav>
             </li>
             <li class="md-nav__item">
              <a class="md-nav__link" href="#cs224n-2019-by-chris-manning">
               5. CS224n-2019-课程笔记 by Chris Manning
              </a>
              <nav class="md-nav">
               <ul class="md-nav__list">
                <li class="md-nav__item">
                 <a class="md-nav__link" href="#id11">
                  5.1. 一些说明和资源
                 </a>
                </li>
                <li class="md-nav__item">
                 <a class="md-nav__link" href="#word-vectors">
                  5.2. 词向量（Word Vectors)
                 </a>
                 <nav class="md-nav">
                  <ul class="md-nav__list">
                   <li class="md-nav__item">
                    <a class="md-nav__link" href="#id15">
                     5.2.1. 自然语言和词义
                    </a>
                   </li>
                   <li class="md-nav__item">
                    <a class="md-nav__link" href="#id17">
                     5.2.2. Word Vectors(词向量)
                    </a>
                   </li>
                   <li class="md-nav__item">
                    <a class="md-nav__link" href="#word2vector">
                     5.2.3. Word2Vector介绍
                    </a>
                    <nav class="md-nav">
                     <ul class="md-nav__list">
                      <li class="md-nav__item">
                       <a class="md-nav__link" href="#id18">
                        5.2.3.1. 主要思想
                       </a>
                      </li>
                     </ul>
                    </nav>
                   </li>
                   <li class="md-nav__item">
                    <a class="md-nav__link" href="#id19">
                     5.2.4. Word2Vector目标函数
                    </a>
                   </li>
                   <li class="md-nav__item">
                    <a class="md-nav__link" href="#id20">
                     5.2.5. 梯度计算推导
                    </a>
                   </li>
                   <li class="md-nav__item">
                    <a class="md-nav__link" href="#id21">
                     5.2.6. word2vector的概览
                    </a>
                   </li>
                   <li class="md-nav__item">
                    <a class="md-nav__link" href="#id22">
                     5.2.7. 优化：梯度下降与随机梯度下降算法的要点
                    </a>
                   </li>
                   <li class="md-nav__item">
                    <a class="md-nav__link" href="#id23">
                     5.2.8. 小结
                    </a>
                   </li>
                  </ul>
                 </nav>
                </li>
                <li class="md-nav__item">
                 <a class="md-nav__link" href="#id24">
                  5.3. 词向量和语义
                 </a>
                 <nav class="md-nav">
                  <ul class="md-nav__list">
                   <li class="md-nav__item">
                    <a class="md-nav__link" href="#review-word2vec">
                     5.3.1. Review：word2vec
                    </a>
                    <nav class="md-nav">
                     <ul class="md-nav__list">
                      <li class="md-nav__item">
                       <a class="md-nav__link" href="#id25">
                        5.3.1.1. 主要思想
                       </a>
                      </li>
                      <li class="md-nav__item">
                       <a class="md-nav__link" href="#id26">
                        5.3.1.2. 关于梯度计算
                       </a>
                      </li>
                      <li class="md-nav__item">
                       <a class="md-nav__link" href="#negative-sample">
                        5.3.1.3. 基于负采样(negative sample)方法计算
                       </a>
                      </li>
                     </ul>
                    </nav>
                   </li>
                   <li class="md-nav__item">
                    <a class="md-nav__link" href="#id27">
                     5.3.2. 基于共现矩阵生成词向量
                    </a>
                   </li>
                   <li class="md-nav__item">
                    <a class="md-nav__link" href="#glove">
                     5.3.3. Glove词向量模型
                    </a>
                   </li>
                   <li class="md-nav__item">
                    <a class="md-nav__link" href="#id29">
                     5.3.4. 怎样评估词向量？
                    </a>
                   </li>
                   <li class="md-nav__item">
                    <a class="md-nav__link" href="#id30">
                     5.3.5. 语义及其歧义性
                    </a>
                   </li>
                   <li class="md-nav__item">
                    <a class="md-nav__link" href="#classification">
                     5.3.6. 分类（Classification）模型知识点回顾
                    </a>
                   </li>
                   <li class="md-nav__item">
                    <a class="md-nav__link" href="#id32">
                     5.3.7. 小结
                    </a>
                   </li>
                  </ul>
                 </nav>
                </li>
                <li class="md-nav__item">
                 <a class="md-nav__link" href="#id33">
                  5.4. 神经网络
                 </a>
                 <nav class="md-nav">
                  <ul class="md-nav__list">
                   <li class="md-nav__item">
                    <a class="md-nav__link" href="#neural-network">
                     5.4.1. Neural NetWork基础
                    </a>
                   </li>
                   <li class="md-nav__item">
                    <a class="md-nav__link" href="#id34">
                     5.4.2. 命名实体识别介绍
                    </a>
                   </li>
                  </ul>
                 </nav>
                </li>
                <li class="md-nav__item">
                 <a class="md-nav__link" href="#bp">
                  5.5. 矩阵计算与BP（反向传播）算法
                 </a>
                 <nav class="md-nav">
                  <ul class="md-nav__list">
                   <li class="md-nav__item">
                    <a class="md-nav__link" href="#id35">
                     5.5.1. 矩阵的梯度计算
                    </a>
                   </li>
                   <li class="md-nav__item">
                    <a class="md-nav__link" href="#id38">
                     5.5.2. BP算法（重点）
                    </a>
                   </li>
                   <li class="md-nav__item">
                    <a class="md-nav__link" href="#id39">
                     5.5.3. 总结
                    </a>
                   </li>
                  </ul>
                 </nav>
                </li>
               </ul>
              </nav>
             </li>
             <li class="md-nav__item">
              <a class="md-nav__extra_link" href="../../_sources/doc/机器智能系列/nlp.rst.txt">
               显示源代码
              </a>
             </li>
            </ul>
           </nav>
           <ul class="md-nav__list">
            <li class="md-nav__item">
             <a class="md-nav__link" href="#id2">
              4.1. 书籍推荐
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="#id3">
              4.2. 什么是 自然语言 理解？
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="#id10">
              4.3. 自然语言技术方向
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="#nlp">
              4.4. NLP国内外优秀学者及实验室
             </a>
            </li>
           </ul>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="#cs224n-2019-by-chris-manning">
            5. CS224n-2019-课程笔记 by Chris Manning
           </a>
           <ul class="md-nav__list">
            <li class="md-nav__item">
             <a class="md-nav__link" href="#id11">
              5.1. 一些说明和资源
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="#word-vectors">
              5.2. 词向量（Word Vectors)
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="#id24">
              5.3. 词向量和语义
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="#id33">
              5.4. 神经网络
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="#bp">
              5.5. 矩阵计算与BP（反向传播）算法
             </a>
            </li>
           </ul>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="kg.html">
            6. 知识图谱(Knowledge Graph)
           </a>
           <ul class="md-nav__list">
            <li class="md-nav__item">
             <a class="md-nav__link" href="kg.html#id1">
              6.1. 资源推荐
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="kg.html#id3">
              6.2. 什么是知识图谱？
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="kg.html#id4">
              6.3. 如何应用知识图谱?
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="kg.html#id5">
              6.4. 如何构建知识图谱？
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="kg.html#id6">
              6.5. 知识图谱案例介绍
             </a>
            </li>
           </ul>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="%E6%A1%86%E6%9E%B6/pytorch.html">
            7. Pytorch
           </a>
           <ul class="md-nav__list">
            <li class="md-nav__item">
             <a class="md-nav__link" href="%E6%A1%86%E6%9E%B6/pytorch.html#id1">
              7.1. 书籍推荐
             </a>
            </li>
           </ul>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="%E6%A1%86%E6%9E%B6/tensorflow.html">
            8. Tensorflow
           </a>
           <ul class="md-nav__list">
            <li class="md-nav__item">
             <a class="md-nav__link" href="%E6%A1%86%E6%9E%B6/tensorflow.html#id1">
              8.1. 书籍推荐
             </a>
            </li>
           </ul>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="%E8%AE%A1%E7%AE%97%E8%AF%AD%E8%A8%80%E5%AD%A6.html">
            9. 计算语言学
           </a>
           <ul class="md-nav__list">
            <li class="md-nav__item">
             <a class="md-nav__link" href="%E8%AE%A1%E7%AE%97%E8%AF%AD%E8%A8%80%E5%AD%A6.html#id2">
              9.1. 前沿跟进
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="%E8%AE%A1%E7%AE%97%E8%AF%AD%E8%A8%80%E5%AD%A6.html#id4">
              9.2. 问题引入【问题驱动学习】
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="%E8%AE%A1%E7%AE%97%E8%AF%AD%E8%A8%80%E5%AD%A6.html#id5">
              9.3. 定义和特点
             </a>
            </li>
           </ul>
          </li>
          <li class="md-nav__item">
           <span class="md-nav__link caption">
            <span class="caption-text">
             开源工具
            </span>
           </span>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../%E5%BC%80%E6%BA%90%E5%B7%A5%E5%85%B7/ETL.html">
            1. ETL工具
           </a>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../%E5%BC%80%E6%BA%90%E5%B7%A5%E5%85%B7/ETL.html#kettle">
            2. Kettle工具使用笔记
           </a>
           <ul class="md-nav__list">
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E5%BC%80%E6%BA%90%E5%B7%A5%E5%85%B7/ETL.html#id1">
              2.1. 基本概念
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E5%BC%80%E6%BA%90%E5%B7%A5%E5%85%B7/ETL.html#id2">
              2.2. 应用场景
             </a>
            </li>
           </ul>
          </li>
          <li class="md-nav__item">
           <span class="md-nav__link caption">
            <span class="caption-text">
             数学物理计算机历史系列
            </span>
           </span>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../%E6%95%B0%E5%AD%A6%E7%89%A9%E7%90%86%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%8E%86%E5%8F%B2%E7%B3%BB%E5%88%97/math.html">
            1. 漫谈数学
           </a>
           <ul class="md-nav__list">
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E6%95%B0%E5%AD%A6%E7%89%A9%E7%90%86%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%8E%86%E5%8F%B2%E7%B3%BB%E5%88%97/math.html#id2">
              1.1. 数学家历史
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E6%95%B0%E5%AD%A6%E7%89%A9%E7%90%86%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%8E%86%E5%8F%B2%E7%B3%BB%E5%88%97/math.html#id7">
              1.2. 怎么理解数学
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E6%95%B0%E5%AD%A6%E7%89%A9%E7%90%86%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%8E%86%E5%8F%B2%E7%B3%BB%E5%88%97/math.html#id8">
              1.3. 怎么掌握数学的思维方式？
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E6%95%B0%E5%AD%A6%E7%89%A9%E7%90%86%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%8E%86%E5%8F%B2%E7%B3%BB%E5%88%97/math.html#id9">
              1.4. 什么是数学最基本的？
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E6%95%B0%E5%AD%A6%E7%89%A9%E7%90%86%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%8E%86%E5%8F%B2%E7%B3%BB%E5%88%97/math.html#id10">
              1.5. 什么问题是好问题？
             </a>
            </li>
           </ul>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../%E6%95%B0%E5%AD%A6%E7%89%A9%E7%90%86%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%8E%86%E5%8F%B2%E7%B3%BB%E5%88%97/physics.html">
            2. 漫谈物理
           </a>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../%E6%95%B0%E5%AD%A6%E7%89%A9%E7%90%86%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%8E%86%E5%8F%B2%E7%B3%BB%E5%88%97/computer.html">
            3. 漫谈计算机
           </a>
          </li>
          <li class="md-nav__item">
           <span class="md-nav__link caption">
            <span class="caption-text">
             团队与项目管理
            </span>
           </span>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../%E5%9B%A2%E9%98%9F%E4%B8%8E%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/team_project.html">
            1. 团队建设与项目管理
           </a>
           <ul class="md-nav__list">
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E5%9B%A2%E9%98%9F%E4%B8%8E%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/team_project.html#project-management">
              1.1. 项目管理（Project Management）
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E5%9B%A2%E9%98%9F%E4%B8%8E%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/team_project.html#id4">
              1.2. 团队协作
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E5%9B%A2%E9%98%9F%E4%B8%8E%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/team_project.html#agile">
              1.3. 敏捷（Agile）开发
             </a>
            </li>
           </ul>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../%E5%9B%A2%E9%98%9F%E4%B8%8E%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/PM.html">
            2. 项目管理（Project Manager， PM）的理论与实践
           </a>
           <ul class="md-nav__list">
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E5%9B%A2%E9%98%9F%E4%B8%8E%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/PM.html#id1">
              2.1. 比较重要的几点
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E5%9B%A2%E9%98%9F%E4%B8%8E%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/PM.html#what-pm">
              2.2. what PM？
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E5%9B%A2%E9%98%9F%E4%B8%8E%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/PM.html#id2">
              2.3. 基本概念
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E5%9B%A2%E9%98%9F%E4%B8%8E%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/PM.html#id3">
              2.4. 项目管理工具
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E5%9B%A2%E9%98%9F%E4%B8%8E%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/PM.html#id4">
              2.5. 沟通
             </a>
            </li>
           </ul>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../%E5%9B%A2%E9%98%9F%E4%B8%8E%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/TecLeader.html">
            3. 技术负责人画像
           </a>
           <ul class="md-nav__list">
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E5%9B%A2%E9%98%9F%E4%B8%8E%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/TecLeader.html#tl">
              3.1. 什么是TL？
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E5%9B%A2%E9%98%9F%E4%B8%8E%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/TecLeader.html#id2">
              3.2. TL的职责？
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E5%9B%A2%E9%98%9F%E4%B8%8E%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/TecLeader.html#id3">
              3.3. 如何做好TL？
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E5%9B%A2%E9%98%9F%E4%B8%8E%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/TecLeader.html#id4">
              3.4. TL如何持续成长？
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E5%9B%A2%E9%98%9F%E4%B8%8E%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/TecLeader.html#id5">
              3.5. TL如何赋能团队
             </a>
            </li>
           </ul>
          </li>
          <li class="md-nav__item">
           <span class="md-nav__link caption">
            <span class="caption-text">
             道与术
            </span>
           </span>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="../%E9%81%93%E4%B8%8E%E6%9C%AF/tao-art.html">
            1. 道与术
           </a>
           <ul class="md-nav__list">
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E9%81%93%E4%B8%8E%E6%9C%AF/tao-art.html#id2">
              1.1. 工作
             </a>
            </li>
            <li class="md-nav__item">
             <a class="md-nav__link" href="../%E9%81%93%E4%B8%8E%E6%9C%AF/tao-art.html#id8">
              1.2. 软实力方法论集合
             </a>
            </li>
           </ul>
          </li>
         </ul>
        </nav>
       </div>
      </div>
     </div>
     <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
      <div class="md-sidebar__scrollwrap">
       <div class="md-sidebar__inner">
        <nav class="md-nav md-nav--secondary">
         <label class="md-nav__title" for="__toc">
          "Contents"
         </label>
         <ul class="md-nav__list" data-md-scrollfix="">
          <li class="md-nav__item">
           <a class="md-nav__link" href="#doc-ji-qi-zhi-neng-xi-lie-nlp--page-root">
            4. 自然语言理解
           </a>
           <nav class="md-nav">
            <ul class="md-nav__list">
             <li class="md-nav__item">
              <a class="md-nav__link" href="#id2">
               4.1. 书籍推荐
              </a>
             </li>
             <li class="md-nav__item">
              <a class="md-nav__link" href="#id3">
               4.2. 什么是 自然语言 理解？
              </a>
              <nav class="md-nav">
               <ul class="md-nav__list">
                <li class="md-nav__item">
                 <a class="md-nav__link" href="#id4">
                  4.2.1. 自然语言？
                 </a>
                </li>
                <li class="md-nav__item">
                 <a class="md-nav__link" href="#id5">
                  4.2.2. 自然语言理解？
                 </a>
                </li>
                <li class="md-nav__item">
                 <a class="md-nav__link" href="#id6">
                  4.2.3. 自然语言的特点
                 </a>
                </li>
                <li class="md-nav__item">
                 <a class="md-nav__link" href="#id7">
                  4.2.4. 难点
                 </a>
                </li>
                <li class="md-nav__item">
                 <a class="md-nav__link" href="#id8">
                  4.2.5. 未来
                 </a>
                </li>
                <li class="md-nav__item">
                 <a class="md-nav__link" href="#id9">
                  4.2.6. 自然语言理解交叉科学
                 </a>
                </li>
               </ul>
              </nav>
             </li>
             <li class="md-nav__item">
              <a class="md-nav__link" href="#id10">
               4.3. 自然语言技术方向
              </a>
             </li>
             <li class="md-nav__item">
              <a class="md-nav__link" href="#nlp">
               4.4. NLP国内外优秀学者及实验室
              </a>
             </li>
            </ul>
           </nav>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__link" href="#cs224n-2019-by-chris-manning">
            5. CS224n-2019-课程笔记 by Chris Manning
           </a>
           <nav class="md-nav">
            <ul class="md-nav__list">
             <li class="md-nav__item">
              <a class="md-nav__link" href="#id11">
               5.1. 一些说明和资源
              </a>
             </li>
             <li class="md-nav__item">
              <a class="md-nav__link" href="#word-vectors">
               5.2. 词向量（Word Vectors)
              </a>
              <nav class="md-nav">
               <ul class="md-nav__list">
                <li class="md-nav__item">
                 <a class="md-nav__link" href="#id15">
                  5.2.1. 自然语言和词义
                 </a>
                </li>
                <li class="md-nav__item">
                 <a class="md-nav__link" href="#id17">
                  5.2.2. Word Vectors(词向量)
                 </a>
                </li>
                <li class="md-nav__item">
                 <a class="md-nav__link" href="#word2vector">
                  5.2.3. Word2Vector介绍
                 </a>
                 <nav class="md-nav">
                  <ul class="md-nav__list">
                   <li class="md-nav__item">
                    <a class="md-nav__link" href="#id18">
                     5.2.3.1. 主要思想
                    </a>
                   </li>
                  </ul>
                 </nav>
                </li>
                <li class="md-nav__item">
                 <a class="md-nav__link" href="#id19">
                  5.2.4. Word2Vector目标函数
                 </a>
                </li>
                <li class="md-nav__item">
                 <a class="md-nav__link" href="#id20">
                  5.2.5. 梯度计算推导
                 </a>
                </li>
                <li class="md-nav__item">
                 <a class="md-nav__link" href="#id21">
                  5.2.6. word2vector的概览
                 </a>
                </li>
                <li class="md-nav__item">
                 <a class="md-nav__link" href="#id22">
                  5.2.7. 优化：梯度下降与随机梯度下降算法的要点
                 </a>
                </li>
                <li class="md-nav__item">
                 <a class="md-nav__link" href="#id23">
                  5.2.8. 小结
                 </a>
                </li>
               </ul>
              </nav>
             </li>
             <li class="md-nav__item">
              <a class="md-nav__link" href="#id24">
               5.3. 词向量和语义
              </a>
              <nav class="md-nav">
               <ul class="md-nav__list">
                <li class="md-nav__item">
                 <a class="md-nav__link" href="#review-word2vec">
                  5.3.1. Review：word2vec
                 </a>
                 <nav class="md-nav">
                  <ul class="md-nav__list">
                   <li class="md-nav__item">
                    <a class="md-nav__link" href="#id25">
                     5.3.1.1. 主要思想
                    </a>
                   </li>
                   <li class="md-nav__item">
                    <a class="md-nav__link" href="#id26">
                     5.3.1.2. 关于梯度计算
                    </a>
                   </li>
                   <li class="md-nav__item">
                    <a class="md-nav__link" href="#negative-sample">
                     5.3.1.3. 基于负采样(negative sample)方法计算
                    </a>
                   </li>
                  </ul>
                 </nav>
                </li>
                <li class="md-nav__item">
                 <a class="md-nav__link" href="#id27">
                  5.3.2. 基于共现矩阵生成词向量
                 </a>
                </li>
                <li class="md-nav__item">
                 <a class="md-nav__link" href="#glove">
                  5.3.3. Glove词向量模型
                 </a>
                </li>
                <li class="md-nav__item">
                 <a class="md-nav__link" href="#id29">
                  5.3.4. 怎样评估词向量？
                 </a>
                </li>
                <li class="md-nav__item">
                 <a class="md-nav__link" href="#id30">
                  5.3.5. 语义及其歧义性
                 </a>
                </li>
                <li class="md-nav__item">
                 <a class="md-nav__link" href="#classification">
                  5.3.6. 分类（Classification）模型知识点回顾
                 </a>
                </li>
                <li class="md-nav__item">
                 <a class="md-nav__link" href="#id32">
                  5.3.7. 小结
                 </a>
                </li>
               </ul>
              </nav>
             </li>
             <li class="md-nav__item">
              <a class="md-nav__link" href="#id33">
               5.4. 神经网络
              </a>
              <nav class="md-nav">
               <ul class="md-nav__list">
                <li class="md-nav__item">
                 <a class="md-nav__link" href="#neural-network">
                  5.4.1. Neural NetWork基础
                 </a>
                </li>
                <li class="md-nav__item">
                 <a class="md-nav__link" href="#id34">
                  5.4.2. 命名实体识别介绍
                 </a>
                </li>
               </ul>
              </nav>
             </li>
             <li class="md-nav__item">
              <a class="md-nav__link" href="#bp">
               5.5. 矩阵计算与BP（反向传播）算法
              </a>
              <nav class="md-nav">
               <ul class="md-nav__list">
                <li class="md-nav__item">
                 <a class="md-nav__link" href="#id35">
                  5.5.1. 矩阵的梯度计算
                 </a>
                </li>
                <li class="md-nav__item">
                 <a class="md-nav__link" href="#id38">
                  5.5.2. BP算法（重点）
                 </a>
                </li>
                <li class="md-nav__item">
                 <a class="md-nav__link" href="#id39">
                  5.5.3. 总结
                 </a>
                </li>
               </ul>
              </nav>
             </li>
            </ul>
           </nav>
          </li>
          <li class="md-nav__item">
           <a class="md-nav__extra_link" href="../../_sources/doc/机器智能系列/nlp.rst.txt">
            显示源代码
           </a>
          </li>
          <li class="md-nav__item" id="searchbox">
          </li>
         </ul>
        </nav>
       </div>
      </div>
     </div>
     <div class="md-content">
      <article class="md-content__inner md-typeset" role="main">
       <section id="id1">
        <h1>
         <span class="section-number">
          4.
         </span>
         自然语言理解
         <a class="headerlink" href="#id1" title="Link to this heading">
          ¶
         </a>
        </h1>
        <div class="admonition note">
         <p class="admonition-title">
          备注
         </p>
         <p>
          更新日期：2020年04月01日
         </p>
        </div>
        <section id="id2">
         <h2>
          <span class="section-number">
           4.1.
          </span>
          书籍推荐
          <a class="headerlink" href="#id2" title="Link to this heading">
           ¶
          </a>
         </h2>
         <ol class="arabic simple">
          <li>
           <dl class="simple">
            <dt>
             <a class="reference external" href="https://book.douban.com/subject/2403834/">
              《Speech and Language Processing, 2nd Edition》
             </a>
            </dt>
            <dd>
             <ul class="simple">
              <li>
               <p>
                内容全面，覆盖面广
               </p>
              </li>
             </ul>
            </dd>
           </dl>
          </li>
          <li>
           <p>
            《统计自然语言处理》 宗成庆
           </p>
          </li>
          <li>
           <p>
            《信息检索导论》
           </p>
          </li>
          <li>
           <p>
            《语言本能-人类语言进化的奥秘》
           </p>
          </li>
         </ol>
        </section>
        <section id="id3">
         <h2>
          <span class="section-number">
           4.2.
          </span>
          什么是 自然语言 理解？
          <a class="headerlink" href="#id3" title="Link to this heading">
           ¶
          </a>
         </h2>
         <section id="id4">
          <h3>
           <span class="section-number">
            4.2.1.
           </span>
           自然语言？
           <a class="headerlink" href="#id4" title="Link to this heading">
            ¶
           </a>
          </h3>
          <ol class="arabic simple">
           <li>
            <p>
             和编程语言相对的语言称为自然语言。
            </p>
           </li>
          </ol>
         </section>
         <section id="id5">
          <h3>
           <span class="section-number">
            4.2.2.
           </span>
           自然语言理解？
           <a class="headerlink" href="#id5" title="Link to this heading">
            ¶
           </a>
          </h3>
          <ol class="arabic simple">
           <li>
            <p>
             目的：使计算机理解人类的自然语言。
            </p>
           </li>
           <li>
            <p>
             本质：结构预测的过程。如输出一句话的句法结构或者语义结构。
            </p>
           </li>
           <li>
            <dl class="simple">
             <dt>
              从无结构化序列预测有结构的语义。
             </dt>
             <dd>
              <ul class="simple">
               <li>
                <p>
                 词性标注、命名实体识别、依存分析、句法分析、指代消解等任务。
                </p>
               </li>
              </ul>
             </dd>
            </dl>
           </li>
           <li>
            <dl class="simple">
             <dt>
              数据驱动的自然语言理解
             </dt>
             <dd>
              <ul class="simple">
               <li>
                <p>
                 深度学习技术的突破
                </p>
               </li>
              </ul>
             </dd>
            </dl>
           </li>
           <li>
            <dl class="simple">
             <dt>
              语义表示（核心）
             </dt>
             <dd>
              <ul class="simple">
               <li>
                <dl class="simple">
                 <dt>
                  one-hot （0/1）也称独热编码
                 </dt>
                 <dd>
                  <ul>
                   <li>
                    <p>
                     表示简单，但局限性很大，如相似度计算。
                    </p>
                   </li>
                  </ul>
                 </dd>
                </dl>
               </li>
               <li>
                <dl class="simple">
                 <dt>
                  分布式语义表示 （空间表示）
                 </dt>
                 <dd>
                  <ul>
                   <li>
                    <p>
                     目前最主要的表示形式。
                    </p>
                   </li>
                   <li>
                    <p>
                     1986年提出的方法。
                    </p>
                   </li>
                  </ul>
                 </dd>
                </dl>
               </li>
              </ul>
             </dd>
            </dl>
           </li>
          </ol>
         </section>
         <section id="id6">
          <h3>
           <span class="section-number">
            4.2.3.
           </span>
           自然语言的特点
           <a class="headerlink" href="#id6" title="Link to this heading">
            ¶
           </a>
          </h3>
          <ol class="arabic simple">
           <li>
            <dl class="simple">
             <dt>
              歧义性多。
             </dt>
             <dd>
              <p>
               <code class="docutils literal notranslate">
                <span class="pre">
                 `
                </span>
                <span class="pre">
                 南京市/长江大桥。南京市长/江大桥。
                </span>
                <span class="pre">
                 `
                </span>
               </code>
               - 关键目标：消除歧义性。
              </p>
             </dd>
            </dl>
           </li>
           <li>
            <dl class="simple">
             <dt>
              递归性
             </dt>
             <dd>
              <p>
               <code class="docutils literal notranslate">
                <span class="pre">
                 `
                </span>
                <span class="pre">
                 你好/不好意思。
                </span>
                <span class="pre">
                 你/好不好意思。
                </span>
                <span class="pre">
                 `
                </span>
               </code>
              </p>
             </dd>
            </dl>
           </li>
           <li>
            <dl class="simple">
             <dt>
              主观性
             </dt>
             <dd>
              <p>
               <code class="docutils literal notranslate">
                <span class="pre">
                 `
                </span>
                <span class="pre">
                 仔细体会这两句：别回了。
                </span>
                <span class="pre">
                 我没事，你忙你的。
                </span>
                <span class="pre">
                 `
                </span>
               </code>
              </p>
             </dd>
            </dl>
           </li>
           <li>
            <dl class="simple">
             <dt>
              社会性
             </dt>
             <dd>
              <ul class="simple">
               <li>
                <p>
                 敬语、语言协调等
                </p>
               </li>
              </ul>
             </dd>
            </dl>
           </li>
          </ol>
         </section>
         <section id="id7">
          <h3>
           <span class="section-number">
            4.2.4.
           </span>
           难点
           <a class="headerlink" href="#id7" title="Link to this heading">
            ¶
           </a>
          </h3>
          <ol class="arabic simple">
           <li>
            <dl class="simple">
             <dt>
              语言的语义表示
             </dt>
             <dd>
              <ul class="simple">
               <li>
                <p>
                 世界（社会/客观）、心智（人/主观）、语言三者相互影响。
                </p>
               </li>
              </ul>
             </dd>
            </dl>
           </li>
           <li>
            <dl class="simple">
             <dt>
              将人类的知识融入到语义表示的过程中。
             </dt>
             <dd>
              <ul class="simple">
               <li>
                <p>
                 人类知识相当于给足了上下文信息
                </p>
               </li>
              </ul>
             </dd>
            </dl>
           </li>
           <li>
            <dl class="simple">
             <dt>
              多模态复杂语境的理解 (说话的表情、手势动作、场景等)
             </dt>
             <dd>
              <ul class="simple">
               <li>
                <p>
                 欢迎/新老/师生/前来/参观！
                </p>
               </li>
               <li>
                <p>
                 欢迎/新老师/生前/来参观！
                </p>
               </li>
              </ul>
             </dd>
            </dl>
           </li>
          </ol>
         </section>
         <section id="id8">
          <h3>
           <span class="section-number">
            4.2.5.
           </span>
           未来
           <a class="headerlink" href="#id8" title="Link to this heading">
            ¶
           </a>
          </h3>
          <ol class="arabic simple">
           <li>
            <p>
             句子消歧。
            </p>
           </li>
           <li>
            <p>
             引入知识。如知识图谱。
            </p>
           </li>
           <li>
            <p>
             多级的跨句子建模。
            </p>
           </li>
           <li>
            <p>
             生成句子更符合当下对话场景。
            </p>
           </li>
           <li>
            <p>
             理解并创作。
            </p>
           </li>
          </ol>
         </section>
         <section id="id9">
          <h3>
           <span class="section-number">
            4.2.6.
           </span>
           自然语言理解交叉科学
           <a class="headerlink" href="#id9" title="Link to this heading">
            ¶
           </a>
          </h3>
          <ol class="arabic simple">
           <li>
            <p>
             计算机科学
            </p>
           </li>
           <li>
            <p>
             脑科学
            </p>
           </li>
           <li>
            <p>
             语言学
            </p>
           </li>
           <li>
            <p>
             语言哲学
            </p>
           </li>
           <li>
            <p>
             心理学、社会学、认知学
            </p>
           </li>
           <li>
            <p>
             神经语言学、汉语言学
            </p>
           </li>
          </ol>
         </section>
        </section>
        <section id="id10">
         <h2>
          <span class="section-number">
           4.3.
          </span>
          自然语言技术方向
          <a class="headerlink" href="#id10" title="Link to this heading">
           ¶
          </a>
         </h2>
         <ul class="simple">
          <li>
           <p>
            基于规则驱动
           </p>
          </li>
          <li>
           <dl class="simple">
            <dt>
             基于数据驱动
            </dt>
            <dd>
             <ul>
              <li>
               <p>
                统计学语言模型
               </p>
              </li>
              <li>
               <p>
                深度语言模型
               </p>
              </li>
             </ul>
            </dd>
           </dl>
          </li>
         </ul>
        </section>
        <section id="nlp">
         <h2>
          <span class="section-number">
           4.4.
          </span>
          NLP国内外优秀学者及实验室
          <a class="headerlink" href="#nlp" title="Link to this heading">
           ¶
          </a>
         </h2>
         <ul class="simple">
          <li>
           <p>
            <a class="reference external" href="https://nlp.stanford.edu/~manning/">
             Christopher Manning
            </a>
           </p>
          </li>
         </ul>
        </section>
       </section>
       <section id="cs224n-2019-by-chris-manning">
        <h1>
         <span class="section-number">
          5.
         </span>
         CS224n-2019-课程笔记 by Chris Manning
         <a class="headerlink" href="#cs224n-2019-by-chris-manning" title="Link to this heading">
          ¶
         </a>
        </h1>
        <div class="admonition tip">
         <p class="admonition-title">
          小技巧
         </p>
         <ul class="simple">
          <li>
           <p>
            <a class="reference external" href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/">
             CS224n-2019
            </a>
            .课程源自斯坦福CS course，2019年发布的自然语言处理，算是NLP的经典吧，老爷子讲的也很风趣幽默。Ok, Hello, everyone!一起来追剧吧。
           </p>
          </li>
          <li>
           <p>
            题外话：课时并不多，所以暗示自己要尽量耐心地把每一节的知识搞懂（慢工出细活，理论知识很重要），自己思考的同时也要动手推导公式甚至编写代码（我就是这么弄得）来刺激大脑理解，难受一阵会发现知识理解很深刻，再回顾此前的知识，豁然开朗！
           </p>
          </li>
         </ul>
        </div>
        <section id="id11">
         <h2>
          <span class="section-number">
           5.1.
          </span>
          一些说明和资源
          <a class="headerlink" href="#id11" title="Link to this heading">
           ¶
          </a>
         </h2>
         <ul class="simple">
          <li>
           <p>
            <a class="reference external" href="http://web.stanford.edu/class/cs224n/index.html">
             课程主页
            </a>
           </p>
          </li>
          <li>
           <p>
            本笔记涉及代码部分将使用pipenv构建虚拟环境。（推荐）
           </p>
          </li>
          <li>
           <dl class="simple">
            <dt>
             课后作业使用PyTocrh框架使用。
            </dt>
            <dd>
             <ul>
              <li>
               <p>
                HW1-HW5.
               </p>
              </li>
              <li>
               <p>
                FP,which is QA .
               </p>
              </li>
             </ul>
            </dd>
           </dl>
          </li>
          <li>
           <p>
            <a class="reference external" href="https://www.bilibili.com/video/BV1Eb411H7Pq/?spm_id_from=333.788.videocard.0)">
             B站资源
            </a>
           </p>
          </li>
          <li>
           <p>
            <a class="reference external" href="https://looperxx.github.io/CS224n-2019-01-Introduction%20and%20Word%20Vectors/">
             笔记参考1
            </a>
           </p>
          </li>
          <li>
           <p>
            <a class="reference external" href="https://www.cnblogs.com/marsggbo/p/10205943.html">
             笔记参考2
            </a>
           </p>
          </li>
         </ul>
        </section>
        <section id="word-vectors">
         <h2>
          <span class="section-number">
           5.2.
          </span>
          词向量（Word Vectors)
          <a class="headerlink" href="#word-vectors" title="Link to this heading">
           ¶
          </a>
         </h2>
         <img alt="../../_images/preface.jpg" src="../../_images/preface.jpg"/>
         <section id="id15">
          <h3>
           <span class="section-number">
            5.2.1.
           </span>
           自然语言和词义
           <a class="headerlink" href="#id15" title="Link to this heading">
            ¶
           </a>
          </h3>
          <ol class="arabic simple">
           <li>
            <dl class="simple">
             <dt>
              自然语言
             </dt>
             <dd>
              <ul class="simple">
               <li>
                <p>
                 你永远无法确定任何单词对他人意味着什么。（中文这个情况就更普遍啦）
                </p>
               </li>
               <li>
                <p>
                 写作是另一件让人类变得强大的事情，这实现了知识的传播和共享。
                </p>
               </li>
              </ul>
             </dd>
            </dl>
           </li>
           <li>
            <dl class="simple">
             <dt>
              语言的意义
             </dt>
             <dd>
              <ul class="simple">
               <li>
                <p>
                 通过一个词或句子等来表达概念
                </p>
               </li>
               <li>
                <p>
                 人们通过文字或声音信号等来表达思想、想法
                </p>
               </li>
               <li>
                <p>
                 在写作、艺术中表达含义
                </p>
               </li>
              </ul>
             </dd>
            </dl>
           </li>
          </ol>
          <div class="highlight-default notranslate">
           <div class="highlight">
            <pre><span></span>一般通过下面这种语言方式进行有意义的思考:
    signifier(symbol)⇔signified(idea or thing) =denotational semantics
</pre>
           </div>
          </div>
          <ol class="arabic" start="3">
           <li>
            <dl>
             <dt>
              语义计算
             </dt>
             <dd>
              <ul>
               <li>
                <dl>
                 <dt>
                  常见方案的不足
                 </dt>
                 <dd>
                  <ul>
                   <li>
                    <dl class="simple">
                     <dt>
                      类似
                      <a class="reference external" href="https://wordnet.princeton.edu/)">
                       WordNet
                      </a>
                      一个面向语义的英语词典，包含上义词（hypernyms）、同义词（synonym sets）。
                     </dt>
                     <dd>
                      <ul class="simple">
                       <li>
                        <p>
                         没有考虑上下文，忽略一个词的细微差别
                        </p>
                       </li>
                       <li>
                        <p>
                         不能及时更新。
                        </p>
                       </li>
                       <li>
                        <p>
                         Can’t compute accurate word similarity
                        </p>
                       </li>
                      </ul>
                     </dd>
                    </dl>
                   </li>
                   <li>
                    <dl>
                     <dt>
                      传统NLP的做法。离散符号表示。one-hot，0-1进行编码：Means one 1, the rest 0s
                     </dt>
                     <dd>
                      <ul class="simple">
                       <li>
                        <p>
                         向量大小就是词汇表的大小（很多无用的信息）
                        </p>
                       </li>
                       <li>
                        <p>
                         无法计算相似度。如下例两个词向量是正交的，点积为0.
                        </p>
                       </li>
                      </ul>
                      <div class="highlight-default notranslate">
                       <div class="highlight">
                        <pre><span></span><span class="n">motel</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">hotel</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span><span class="p">]</span>
</pre>
                       </div>
                      </div>
                     </dd>
                    </dl>
                   </li>
                  </ul>
                 </dd>
                </dl>
               </li>
               <li>
                <dl class="simple">
                 <dt>
                  提取新方案
                 </dt>
                 <dd>
                  <ul class="simple">
                   <li>
                    <p>
                     Could try to rely on WordNet’s list of synonyms to get similarity?
                    </p>
                   </li>
                   <li>
                    <p>
                     learn to encode similarity in the vectors themselves（学习词自身的编码信息）
                    </p>
                   </li>
                  </ul>
                 </dd>
                </dl>
               </li>
              </ul>
             </dd>
            </dl>
           </li>
           <li>
            <dl class="simple">
             <dt>
              通过上下文表示词
             </dt>
             <dd>
              <ul class="simple">
               <li>
                <p>
                 分布式语义：一个词的含义往往是由附近高频出现的词决定的。
                </p>
               </li>
               <li>
                <p>
                 word出现在文本中，这个Word周围会有由词的集合组成的Context出现。这个上下文是固定一个窗口size的。
                </p>
               </li>
               <li>
                <dl class="simple">
                 <dt>
                  我们可以使用存在Word的大量
                  <a class="reference external" href="http://ling.cuc.edu.cn/RawPub/)">
                   语料
                  </a>
                  来学习其向量表示。比如学习“中国科学院”词（实际中会学习每个词），在下列的语料中。
                 </dt>
                 <dd>
                  <ol class="arabic simple">
                   <li>
                    <p>
                     先向获得2009年度国家最高科学技术奖的
                     <strong>
                      中国科学院
                     </strong>
                     院士、复旦大学数学研究所名誉所长谷超豪和
                    </p>
                   </li>
                   <li>
                    <p>
                     院士、复旦大学数学研究所名誉所长谷超豪和
                     <strong>
                      中国科学院
                     </strong>
                     院士、中国航天科技集团公司高级技术顾
                    </p>
                   </li>
                   <li>
                    <p>
                     大国”向“造船强国”迈进。 由
                     <strong>
                      中国科学院
                     </strong>
                     和上海市政府共同建设的上海同步辐射光源工
                    </p>
                   </li>
                   <li>
                    <p>
                     丽；河南卓越工程管理有限公司董事长邬敏
                     <strong>
                      中国科学院
                     </strong>
                     研究生院教授杨佳十人“全国三八红旗手
                    </p>
                   </li>
                  </ol>
                 </dd>
                </dl>
               </li>
              </ul>
             </dd>
            </dl>
           </li>
          </ol>
         </section>
         <section id="id17">
          <h3>
           <span class="section-number">
            5.2.2.
           </span>
           Word Vectors(词向量)
           <a class="headerlink" href="#id17" title="Link to this heading">
            ¶
           </a>
          </h3>
          <ul class="simple">
           <li>
            <p>
             根据一个的词的上下文，来为词构建密集的向量，以使得该向量与出现在类似上下文中的词相似
            </p>
           </li>
           <li>
            <dl class="simple">
             <dt>
              引出词向量，也称词嵌入或词表示。
             </dt>
             <dd>
              <ul>
               <li>
                <p>
                 word vectors are sometimes called
                 <strong>
                  word embeddings
                 </strong>
                 or word representations.
                </p>
               </li>
               <li>
                <p>
                 They are a
                 <strong>
                  distributed representation
                 </strong>
                 .
                </p>
               </li>
               <li>
                <p>
                 例如“中国”这个词经过训练后的词向量为：
                </p>
               </li>
              </ul>
             </dd>
            </dl>
           </li>
          </ul>
          <div class="math notranslate nohighlight">
           \[\begin{split}中国 = \begin{pmatrix} 0.286\\
            0.792\\
            −0.177\\
            −0.107\\
            0.109\\
            −0.542\\
            0.349\\
            0.271 \end{pmatrix}\end{split}\]
          </div>
         </section>
         <section id="word2vector">
          <h3>
           <span class="section-number">
            5.2.3.
           </span>
           Word2Vector介绍
           <a class="headerlink" href="#word2vector" title="Link to this heading">
            ¶
           </a>
          </h3>
          <div class="admonition note">
           <p class="admonition-title">
            备注
           </p>
           <p>
            Word2vec (Mikolov et al. 2013) 是一种学习词向量的
            <em>
             框架
            </em>
            。
           </p>
          </div>
          <section id="id18">
           <h4>
            <span class="section-number">
             5.2.3.1.
            </span>
            主要思想
            <a class="headerlink" href="#id18" title="Link to this heading">
             ¶
            </a>
           </h4>
           <ol class="arabic simple">
            <li>
             <p>
              我们有个比较大的文本数据集。
             </p>
            </li>
            <li>
             <p>
              文本中的每个词通过一个固定长度的词向量表示。
             </p>
            </li>
            <li>
             <p>
              扫描文本中每一个位置
              <strong>
               t
              </strong>
              所表示的词,其中有一个中心词
              <strong>
               c
              </strong>
              和上下文词
              <strong>
               o
              </strong>
              。
             </p>
            </li>
            <li>
             <p>
              通过c和o的词向量的相似性，计算在给定c,即中心词来计算o,即上下文的概率。反之亦然。
             </p>
            </li>
            <li>
             <p>
              不断调整词向量来最大化上面提到的概率。
             </p>
            </li>
           </ol>
           <ul>
            <li>
             <p>
              举例如下
             </p>
             <blockquote>
              <div>
               <img alt="../../_images/w2v_ex-1.png" src="../../_images/w2v_ex-1.png"/>
               <img alt="../../_images/w2v_ex-2.png" src="../../_images/w2v_ex-2.png"/>
              </div>
             </blockquote>
            </li>
           </ul>
          </section>
         </section>
         <section id="id19">
          <h3>
           <span class="section-number">
            5.2.4.
           </span>
           Word2Vector目标函数
           <a class="headerlink" href="#id19" title="Link to this heading">
            ¶
           </a>
          </h3>
          <ol class="arabic">
           <li>
            <p>
             思路(后面要说的Skip-grams模型）
            </p>
            <blockquote>
             <div>
              <p>
               在每个位置
               <span class="math notranslate nohighlight">
                \(t\)
               </span>
               （t = 1，……，T)，给定一个中心词
               <span class="math notranslate nohighlight">
                \(w_j\)
               </span>
               和一段固定长度的窗口
               <span class="math notranslate nohighlight">
                \(m\)
               </span>
               ，预测上下文中每个单词的概率。
              </p>
             </div>
            </blockquote>
           </li>
          </ol>
          <div class="math notranslate nohighlight">
           \[ \begin{align}\begin{aligned}\begin{split}Likelihood = L(\theta) = \prod_{t=1}^{T}\prod _{\substack{-m\leq j \leq m \\ j\neq 0}}P(w_{t+j}|w_t;\theta)\end{split}\\其中 \theta 是一个需要全局优化的变量\end{aligned}\end{align} \]
          </div>
          <ul>
           <li>
            <p>
             目标函数
             <span class="math notranslate nohighlight">
              \(J(\theta)\)
             </span>
             （也称为
             <strong>
              代价或损失函数
             </strong>
             ）,是一个负对数似然：
            </p>
            <blockquote>
             <div>
              <div class="math notranslate nohighlight">
               \[\begin{split}J(\theta) = -\frac{1}{T}logL(\theta) = -\frac{1}{T}\sum_{t=1}^{T}\sum _{\substack{-m\leq j \leq m \\ j\neq 0}}P(w_{t+j}|w_t;\theta)\end{split}\]
              </div>
             </div>
            </blockquote>
           </li>
          </ul>
          <p>
           Q1： 如何计算
           <span class="math notranslate nohighlight">
            \(P(w_{t+j}|w_t;\theta)\)
           </span>
           ?
          </p>
          <p>
           A1： 每个词w用两个向量表示
          </p>
          <blockquote>
           <div>
            <ul class="simple">
             <li>
              <p>
               当w是中心词时用向量
               <span class="math notranslate nohighlight">
                \(v_w\)
               </span>
               表示
              </p>
             </li>
             <li>
              <p>
               当w是上下文词时用向量
               <span class="math notranslate nohighlight">
                \(u_w\)
               </span>
               表示
              </p>
             </li>
            </ul>
           </div>
          </blockquote>
          <p>
           那么对于一个中心词c和上下文词o可用如下形式表示
          </p>
          <blockquote>
           <div>
            <div class="math notranslate nohighlight">
             \[P(o|c) = \frac{exp(u_o^T v_c)}{\sum_{w\in V} exp(u_w^Tv_c)}\]
            </div>
            <p>
             其中，
             <span class="math notranslate nohighlight">
              \(u_o^T v_c`点积表示o和c的相似度，点积越大则表示概率越大；:math:`{\sum exp(u_w^Tv_c)}\)
             </span>
             目的是为了对整个词汇表进行标准化。
            </p>
           </div>
          </blockquote>
          <ul>
           <li>
            <dl>
             <dt>
              softmax function
             </dt>
             <dd>
              <p>
               softmax函数作用是将任意标量
               <span class="math notranslate nohighlight">
                \(x_i`映射为概率分布 :math:`p_i\)
               </span>
               。
              </p>
              <div class="math notranslate nohighlight">
               \[softmax(x_i) = \frac{exp(x_i)}{\sum_{j=1}^{n} exp(u_w^Tv_c)} = p_i\]
              </div>
              <ul class="simple">
               <li>
                <p>
                 “max"对比较大的
                 <span class="math notranslate nohighlight">
                  \(x_i\)
                 </span>
                 映射比较大的概率
                </p>
               </li>
               <li>
                <p>
                 ”soft" 对那些小的
                 <span class="math notranslate nohighlight">
                  \(x_i\)
                 </span>
                 也会给予一定概率
                </p>
               </li>
               <li>
                <p>
                 这是一种常见的操作，如深度学习
                </p>
               </li>
              </ul>
             </dd>
            </dl>
           </li>
          </ul>
          <div class="admonition tip">
           <p class="admonition-title">
            小技巧
           </p>
           <ul class="simple">
            <li>
             <p>
              利用对数的特性将目标函数转换为对数求和，减少计算的复杂度。
             </p>
            </li>
            <li>
             <p>
              最小化目标函数 ⟺最大化预测的准确率
             </p>
            </li>
           </ul>
          </div>
          <ul class="simple">
           <li>
            <p>
             通过不断的优化参数最小化误差来训练模型。
            </p>
           </li>
           <li>
            <dl class="simple">
             <dt>
              为了训练模型，需要计算所有向量的梯度
             </dt>
             <dd>
              <ul>
               <li>
                <p>
                 <span class="math notranslate nohighlight">
                  \(\theta\)
                 </span>
                 用一个很长的向量表示所有模型的参数。
                </p>
               </li>
               <li>
                <dl class="simple">
                 <dt>
                  每个单词有个两个向量。
                 </dt>
                 <dd>
                  <ul>
                   <li>
                    <p>
                     Why two vectors? àEasier optimization. Average both at the end.
                    </p>
                   </li>
                  </ul>
                 </dd>
                </dl>
               </li>
               <li>
                <p>
                 利用不断移动的梯度来优化这些模型的参数。
                </p>
               </li>
              </ul>
             </dd>
            </dl>
           </li>
          </ul>
         </section>
         <section id="id20">
          <h3>
           <span class="section-number">
            5.2.5.
           </span>
           梯度计算推导
           <a class="headerlink" href="#id20" title="Link to this heading">
            ¶
           </a>
          </h3>
          <p>
           下面开始推导
           <span class="math notranslate nohighlight">
            \(P(w_{t+j}|w_t;\theta)\)
           </span>
           :
对
           <span class="math notranslate nohighlight">
            \(v_c\)
           </span>
           求偏微分
          </p>
          <div class="math notranslate nohighlight">
           \[ \begin{align}\begin{aligned}\begin{split}\frac{\partial}{\partial v_c }logP(o|c)
&amp;= \frac{\partial}{\partial v_c }log\frac{exp(u_o^T v_c)}{\sum_{w\in V} exp(u_w^Tv_c)}\\
&amp;=\frac{\partial}{\partial v_c}\left(\log \exp(u_o^Tv_c)-\log{\sum_{w\in V}\exp(u_w^Tv_c)}\right)\\
&amp;=\frac{\partial}{\partial v_c}\left(u_o^Tv_c-\log{\sum_{w\in V}\exp(u_w^Tv_c)}\right)\\
&amp;=u_o-\frac{\sum_{w\in V}\exp(u_w^Tv_c)u_w}{\sum_{w\in V}\exp(u_w^Tv_c)}\\
&amp;=u_o-\sum_{w\in V}\frac{\exp(u_w^Tv_c)}{\sum_{w\in V}\exp(u_w^Tv_c)}u_w\\
&amp;=u_o-\sum_{w\in V}P(w|c)u_w\\\end{split}\\\begin{split}其中，u_o是我们观测到每个词的值，\sum_{w\in V}P(w|c)u_w是模型的预测值，\\
利用梯度下降不断使两者更为接近，使偏微为0.\end{split}\end{aligned}\end{align} \]
          </div>
          <p>
           还有对
           <span class="math notranslate nohighlight">
            \(u_o\)
           </span>
           的偏微过程，大家动手推导下，比较简单的。
          </p>
          <div class="admonition tip">
           <p class="admonition-title">
            小技巧
           </p>
           <p>
            补充一点边角知识，在上面的推导过程中用的到：
           </p>
           <ul class="simple">
            <li>
             <p>
              向量函数与其导数
              <span class="math notranslate nohighlight">
               \(\frac{\partial Ax}{\partial x} = A^T, \frac{\partial x^T A}{\partial x} = A\)
              </span>
             </p>
            </li>
            <li>
             <p>
              链式法则：
              <span class="math notranslate nohighlight">
               \(log'f[g(x)] = \frac{1}{f[g(x)]}g'(x)\)
              </span>
             </p>
            </li>
           </ul>
          </div>
         </section>
         <section id="id21">
          <h3>
           <span class="section-number">
            5.2.6.
           </span>
           word2vector的概览
           <a class="headerlink" href="#id21" title="Link to this heading">
            ¶
           </a>
          </h3>
          <p>
           前面提到的Word2Vector是一种学习词向量的框架（模型），它包含两个实现算法：
          </p>
          <ol class="arabic">
           <li>
            <p>
             Skip-grams (SG) （课上讲的就类似这种）
            </p>
            <blockquote>
             <div>
              <ul class="simple">
               <li>
                <p>
                 根据中心词周围的上下文单词来预测该词的词向量
                </p>
               </li>
              </ul>
             </div>
            </blockquote>
           </li>
           <li>
            <p>
             Continuous Bag of Words (CBOW)
            </p>
            <blockquote>
             <div>
              <ul class="simple">
               <li>
                <p>
                 根据中心词预测周围上下文的词的概率分布。
                </p>
               </li>
              </ul>
             </div>
            </blockquote>
           </li>
          </ol>
          <p>
           另外提到两个训练的方式：
          </p>
          <ol class="arabic">
           <li>
            <p>
             negative sampling (比较简单的方式)
            </p>
            <blockquote>
             <div>
              <ul class="simple">
               <li>
                <p>
                 通过抽取负样本来定义目标
                </p>
               </li>
              </ul>
             </div>
            </blockquote>
           </li>
           <li>
            <p>
             hierarchical softmax
            </p>
            <blockquote>
             <div>
              <ul class="simple">
               <li>
                <p>
                 通过使用一个树来计算所有词的概率来定义目标。
                </p>
               </li>
              </ul>
             </div>
            </blockquote>
           </li>
          </ol>
         </section>
         <section id="id22">
          <h3>
           <span class="section-number">
            5.2.7.
           </span>
           优化：梯度下降与随机梯度下降算法的要点
           <a class="headerlink" href="#id22" title="Link to this heading">
            ¶
           </a>
          </h3>
          <ul>
           <li>
            <dl>
             <dt>
              梯度下降（Gradient Descent，GD）
             </dt>
             <dd>
              <ol class="arabic simple">
               <li>
                <p>
                 最小化的目标（代价）函数
                 <span class="math notranslate nohighlight">
                  \(J(\theta）\)
                 </span>
                </p>
               </li>
               <li>
                <p>
                 使用梯度下降算法去优化
                 <span class="math notranslate nohighlight">
                  \(J(\theta）\)
                 </span>
                </p>
               </li>
               <li>
                <p>
                 对于当前
                 <span class="math notranslate nohighlight">
                  \(\theta\)
                 </span>
                 采用一个合适的步长（学习率）不断重复计算
                 <span class="math notranslate nohighlight">
                  \(J(\theta）\)
                 </span>
                 的梯度，朝着负向梯度方向。
                </p>
               </li>
              </ol>
              <img alt="../../_images/SG.jpg" src="../../_images/SG.jpg"/>
              <ol class="arabic" start="4">
               <li>
                <p>
                 更新等式（矩阵）
                </p>
                <blockquote>
                 <div>
                  <div class="math notranslate nohighlight">
                   \[ \begin{align}\begin{aligned}\theta^{new} = \theta^{old} - \alpha\nabla_\theta J(\theta)\\其中，\theta = 步长（学习率）\end{aligned}\end{align} \]
                  </div>
                 </div>
                </blockquote>
               </li>
               <li>
                <dl>
                 <dt>
                  更新等式（单个参数）
                 </dt>
                 <dd>
                  <div class="math notranslate nohighlight">
                   \[\theta_j^{new} = \theta_j^{old} - \alpha\frac{\partial}{\partial \theta_j^{old}}J(\theta)\]
                  </div>
                 </dd>
                </dl>
               </li>
              </ol>
             </dd>
            </dl>
           </li>
           <li>
            <dl class="simple">
             <dt>
              随机梯度下降（Stochastic Gradient Descen, SGD）
             </dt>
             <dd>
              <ul class="simple">
               <li>
                <dl class="simple">
                 <dt>
                  目的
                 </dt>
                 <dd>
                  <p>
                   进一步解决
                   <span class="math notranslate nohighlight">
                    \(J(\theta）\)
                   </span>
                   的训练效率（因为目标函数包含所有的参数，而且数据集一般都是很大的）问题：太慢了。
                  </p>
                 </dd>
                </dl>
               </li>
               <li>
                <p>
                 Repeatedly sample windows, and update after each one
                </p>
               </li>
              </ul>
             </dd>
            </dl>
           </li>
          </ul>
         </section>
         <section id="id23">
          <h3>
           <span class="section-number">
            5.2.8.
           </span>
           小结
           <a class="headerlink" href="#id23" title="Link to this heading">
            ¶
           </a>
          </h3>
          <ul class="simple">
           <li>
            <p>
             本节首先从语言的语义问题开始讲起，然后为了表示语义，引出了词向量概念，接着着重讲了Word2Vector框架、原理、算法推导等，最后简单提了下目标函数的优化的方式。
            </p>
           </li>
           <li>
            <dl class="simple">
             <dt>
              看完并梳理完本节知识，我产生了几个问题：
             </dt>
             <dd>
              <ul>
               <li>
                <p>
                 词向量提了好多次，那么每个词的词向量究竟是如何产生（计算）的呢？存在哪些方法？
                </p>
               </li>
               <li>
                <dl class="simple">
                 <dt>
                  有几个点的原理还需进一步深入理解：
                 </dt>
                 <dd>
                  <ul>
                   <li>
                    <p>
                     负采样、层次采样；区别和前后的优势在哪里？
                    </p>
                   </li>
                   <li>
                    <p>
                     SG、CBOW算法的细节；本质区别和各自优势是什么？
                    </p>
                   </li>
                  </ul>
                 </dd>
                </dl>
               </li>
              </ul>
             </dd>
            </dl>
           </li>
          </ul>
         </section>
        </section>
        <section id="id24">
         <h2>
          <span class="section-number">
           5.3.
          </span>
          词向量和语义
          <a class="headerlink" href="#id24" title="Link to this heading">
           ¶
          </a>
         </h2>
         <div class="admonition note">
          <p class="admonition-title">
           备注
          </p>
          <p>
           本节课后，我们就能够开始读一些关于词嵌入方面的论文了。
          </p>
         </div>
         <img alt="../../_images/preface-2.png" src="../../_images/preface-2.png"/>
         <section id="review-word2vec">
          <h3>
           <span class="section-number">
            5.3.1.
           </span>
           Review：word2vec
           <a class="headerlink" href="#review-word2vec" title="Link to this heading">
            ¶
           </a>
          </h3>
          <section id="id25">
           <h4>
            <span class="section-number">
             5.3.1.1.
            </span>
            主要思想
            <a class="headerlink" href="#id25" title="Link to this heading">
             ¶
            </a>
           </h4>
           <ul class="simple">
            <li>
             <dl class="simple">
              <dt>
               这里以skip-gram）模型为例
              </dt>
              <dd>
               <ol class="arabic simple">
                <li>
                 <p>
                  遍历整个语料库的每个词，通过中心词向量预测周围的词向量
                 </p>
                </li>
                <li>
                 <p>
                  算法学到的词向量能用来计算词的相似度或语义等相关需求。
                 </p>
                </li>
               </ol>
              </dd>
             </dl>
            </li>
           </ul>
          </section>
          <section id="id26">
           <h4>
            <span class="section-number">
             5.3.1.2.
            </span>
            关于梯度计算
            <a class="headerlink" href="#id26" title="Link to this heading">
             ¶
            </a>
           </h4>
           <ul class="simple">
            <li>
             <p>
              GD。计算效率低，每次对所有样本进行梯度计算
             </p>
            </li>
            <li>
             <p>
              SGD。每次只对一个固定大小的样本窗口进行更新，效率较高。
             </p>
            </li>
            <li>
             <dl class="simple">
              <dt>
               梯度计算存在稀疏性（0比较多）
              </dt>
              <dd>
               <ul>
                <li>
                 <p>
                  But in each window, we only have at most 2m + 1 words, so it is very sparse!
                 </p>
                </li>
                <li>
                 <dl class="simple">
                  <dt>
                   解决方案：
                  </dt>
                  <dd>
                   <ul>
                    <li>
                     <p>
                      only update certain rows of full embedding matrices U and V. (使用稀疏矩阵仅更新稀疏性低的词向量矩阵U和V)
                     </p>
                    </li>
                    <li>
                     <p>
                      you need to keep around a hash for word vectors （使用hash来更新，即k-v，k表示word,v表示其词向量）
                     </p>
                    </li>
                   </ul>
                  </dd>
                 </dl>
                </li>
               </ul>
              </dd>
             </dl>
            </li>
           </ul>
          </section>
          <section id="negative-sample">
           <h4>
            <span class="section-number">
             5.3.1.3.
            </span>
            基于负采样(negative sample)方法计算
            <a class="headerlink" href="#negative-sample" title="Link to this heading">
             ¶
            </a>
           </h4>
           <ol class="arabic">
            <li>
             <dl>
              <dt>
               计算下列式子：
              </dt>
              <dd>
               <div class="math notranslate nohighlight">
                \[P(o|c) = \frac{exp(u_o^T v_c)}{\sum_{w\in V} exp(u_w^Tv_c)}\]
               </div>
               <p>
                其中 :math:` {sum_{win V} exp(u_w^Tv_c)}` 计算代价非常大（整个语料库计算），如何降低这一块的计算复杂度就是需要考虑的问题。
               </p>
              </dd>
             </dl>
            </li>
            <li>
             <dl class="simple">
              <dt>
               负采样方法介绍
              </dt>
              <dd>
               <ul class="simple">
                <li>
                 <p>
                  主要思想：train binary logistic regressions。除了对中心词窗口大小附近的上下文词取样以外(即true pairs)，还会随机抽取一些噪声和中心词配对（即noise pairs）进行计算，而不是遍历整个词库。
                 </p>
                </li>
                <li>
                 <p>
                  这个
                  <strong>
                   负
                  </strong>
                  指的是噪声数据（无关的语料词 noise pairs）
                 </p>
                </li>
               </ul>
              </dd>
             </dl>
            </li>
            <li>
             <p>
              负采样计算细节
             </p>
            </li>
           </ol>
           <div class="admonition tip">
            <p class="admonition-title">
             小技巧
            </p>
            <p>
             可参考论文
             <a class="reference external" href="https://github.com/lihanghang/NLP-Knowledge-Graph/raw/master/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E8%AF%AD%E8%A8%80%E8%A1%A8%E7%A4%BA%E6%A8%A1%E5%9E%8B/Tomas%20Mikolov%20papers/Distributed%20representations%20of%20words%20and%20phrases%20and%20their%20compositionality.pdf">
              Distributed Representations of Words and Phrases and their Compositionality” (Mikolov et al. 2013)
             </a>
             第3页。
            </p>
           </div>
           <ul>
            <li>
             <p>
              最大化下面的目标函数
             </p>
             <blockquote>
              <div>
               <div class="math notranslate nohighlight">
                \[J(\theta) = \frac{1}{T}\sum_{t=1}^{T}J_{t}(\theta)\]
               </div>
               <div class="math notranslate nohighlight">
                \[ \begin{align}\begin{aligned}J_{t}(\theta)=\log \sigma(u_{o}^{T} v_{c})+\sum_{i=1}^{k} \mathbb{E}_{j \sim P(w)}[\log \sigma(-u_{j}^{T} v_{c})]\\其中，\sigma(x)=\frac{1}{1+e^{-x}}\end{aligned}\end{align} \]
               </div>
               <ul class="simple">
                <li>
                 <p>
                  公式第一项表示最大化真实的中心词和其上下文词的概率；第二项是最小化负采样的噪声值（中心词及其上下文）的概率，j表示负采样的样本，并以P(w)大小进行随机采样。
                 </p>
                </li>
               </ul>
               <div class="admonition note">
                <p class="admonition-title">
                 备注
                </p>
                <ul class="simple">
                 <li>
                  <p>
                   P(w)，这里使用了N元统计模型且N取1，即一元统计模型（unigram），表示每个词都和其它词独立，和它的上下文无关。每个位置上的词都是从多项分布独立生成的。
                  </p>
                 </li>
                 <li>
                  <p>
                   补充N元统计模型，N=2时就为二元统计模型，即每个词和其前1个词有关。一般的，假设每个词
                   <span class="math notranslate nohighlight">
                    \(x_t\)
                   </span>
                   只依赖于其前面的n−1个词（n 阶马尔可夫性质）。
                  </p>
                 </li>
                 <li>
                  <p>
                   通过N元统计模型，我们可以计算一个序列的概率，从而判断该序列是否符合自然语言的语法和语义规则。
                  </p>
                 </li>
                 <li>
                  <p>
                   这个方法在构建词向量时最大的问题就是
                   <strong>
                    数据稀疏性
                   </strong>
                   ，大家可以思考下为什么？还能想到改进或其他更好的方法？
                  </p>
                 </li>
                </ul>
               </div>
              </div>
             </blockquote>
            </li>
           </ul>
          </section>
         </section>
         <section id="id27">
          <h3>
           <span class="section-number">
            5.3.2.
           </span>
           基于共现矩阵生成词向量
           <a class="headerlink" href="#id27" title="Link to this heading">
            ¶
           </a>
          </h3>
          <p>
           But why not capture co-occurrence counts directly?
1. 主要思想
          </p>
          <blockquote>
           <div>
            <ul>
             <li>
              <dl>
               <dt>
                有一个共现矩阵x，其可选的粒度有两种：固定窗口大小的window、文档级的document
               </dt>
               <dd>
                <ul>
                 <li>
                  <dl>
                   <dt>
                    window级别的共现矩阵： 类似有word2vector，利用每个词使用固定的窗口大小来获取其语法或语义信息。
                   </dt>
                   <dd>
                    <ul>
                     <li>
                      <dl>
                       <dt>
                        例如，window_len = 1的单词与单词同时出现的次数来产生基于窗口的co-occurrence matrix。
                       </dt>
                       <dd>
                        <ul class="simple">
                         <li>
                          <p>
                           语料： I like deep learning. I like NLP. I enjoy flying. c
                          </p>
                         </li>
                        </ul>
                        <img alt="../../_images/co_matrix.png" src="../../_images/co_matrix.png"/>
                       </dd>
                      </dl>
                     </li>
                     <li>
                      <p>
                       解释下上述矩阵的含义：按窗口为1，同时出现的原则，I I语料没有这样的表达即同时出现的次数为0，I like 前两条都出现了，即为2。
                      </p>
                     </li>
                     <li>
                      <p>
                       简单观察矩阵，就会发现很稀疏，存在大量的0，而且随着语料库的增大，维数也会增大，这显然不是最佳方法，得想办法增加空间的利用率，目标就是稀疏变稠密，以免因稀疏性对下游任务造成影响。
                      </p>
                     </li>
                    </ul>
                   </dd>
                  </dl>
                 </li>
                 <li>
                  <p>
                   document级别的共现矩阵：基本假设是文档若存在相关联，则其会出现同样的单词。一般使用“Latent Semantic Analysis”（LSA)潜在语义分析方法进行矩阵的生成
                  </p>
                 </li>
                </ul>
               </dd>
              </dl>
             </li>
            </ul>
           </div>
          </blockquote>
          <ol class="arabic" start="2">
           <li>
            <p>
             问题：怎么对共现矩阵进行降维呢？
            </p>
            <blockquote>
             <div>
              <ul>
               <li>
                <p>
                 Singular Value Decomposition（SVD)奇异值分解
                </p>
                <blockquote>
                 <div>
                  <ul class="simple">
                   <li>
                    <p>
                     将矩阵X分解为
                     <span class="math notranslate nohighlight">
                      \(U \Sigma V^{\top} \Sigma\)
                     </span>
                     是对角阵，其主对角线的每个值都是奇异值；U和
                     <span class="math notranslate nohighlight">
                      \(V^{\top}\)
                     </span>
                     两个正交矩阵
                    </p>
                   </li>
                  </ul>
                  <img alt="../../_images/SVD.png" src="../../_images/SVD.png"/>
                  <div class="admonition note">
                   <p class="admonition-title">
                    备注
                   </p>
                   <ol class="arabic simple">
                    <li>
                     <p>
                      SVD就是将一个线性变换分解为两个线性变换，一个线性变换代表旋转，一个线性变换代表拉伸。
                     </p>
                    </li>
                    <li>
                     <p>
                      正交矩阵对应的变换是旋转变换，对角矩阵对应的变换是伸缩变换。
                     </p>
                    </li>
                    <li>
                     <p>
                      这里我们可以再联想对比另外一个经典的降维算法PCA。
                     </p>
                    </li>
                   </ol>
                  </div>
                 </div>
                </blockquote>
               </li>
              </ul>
             </div>
            </blockquote>
           </li>
           <li>
            <p>
             Hacks to X
            </p>
            <blockquote>
             <div>
              <ul class="simple">
               <li>
                <dl class="simple">
                 <dt>
                  按比例缩放计数会有很大帮助。（怎么讲？）哦，就是对一些高频的功能性（has、the等）词进行缩放，缩放后不会影响句法结构或者语义等
                 </dt>
                 <dd>
                  <ul>
                   <li>
                    <p>
                     min(X,t), with t ≈ 100
                    </p>
                   </li>
                   <li>
                    <p>
                     忽略这些词
                    </p>
                   </li>
                  </ul>
                 </dd>
                </dl>
               </li>
               <li>
                <p>
                 定义一个Ramped windows，统计距离很小（关联度高）的的词。（距离越小，代表关联度越高）
                </p>
               </li>
               <li>
                <p>
                 使用皮尔逊相关系数来替换直接计数方式，并设无关联值为0。
                </p>
               </li>
              </ul>
              <div class="admonition note">
               <p class="admonition-title">
                备注
               </p>
               <p>
                Pearson相关系数是衡量向量相似度的一种方法。输出范围为-1到+1, 0代表无相关性，负值为负相关，正值为正相关。
               </p>
              </div>
             </div>
            </blockquote>
           </li>
          </ol>
         </section>
         <section id="glove">
          <h3>
           <span class="section-number">
            5.3.3.
           </span>
           Glove词向量模型
           <a class="headerlink" href="#glove" title="Link to this heading">
            ¶
           </a>
          </h3>
          <div class="admonition tip">
           <p class="admonition-title">
            小技巧
           </p>
           <p>
            可参考阅读
            <a class="reference external" href="https://nlp.stanford.edu/projects/glove/">
             Glove
            </a>
            主页
           </p>
          </div>
          <ol class="arabic">
           <li>
            <p>
             基于计数和直接预测的比较
            </p>
            <blockquote>
             <div>
              <img alt="../../_images/count_predict.png" src="../../_images/count_predict.png"/>
              <ul class="simple">
               <li>
                <p>
                 基于计数（统计理论）
                </p>
               </li>
               <li>
                <p>
                 直接预测（概率模型或神经网络）
                </p>
               </li>
              </ul>
              <p>
               <strong>
                到底哪一个方法是正法呢，还是说走向合作呢？答案是合作，相互借鉴才能发挥更大价值（1+1&gt;2)
               </strong>
              </p>
             </div>
            </blockquote>
           </li>
           <li>
            <dl>
             <dt>
              探索在向量间的差异中挖掘语义
             </dt>
             <dd>
              <ul class="simple">
               <li>
                <p>
                 方向：能在共现矩阵的概率的比值中看出语义相关
                </p>
               </li>
              </ul>
              <img alt="../../_images/co_matrix_ratio.png" src="../../_images/co_matrix_ratio.png"/>
              <ul class="simple">
               <li>
                <dl class="simple">
                 <dt>
                  解释下表所传递的信息：当需知道ice冰和steam气的关系时，可借助词k：
                 </dt>
                 <dd>
                  <ul>
                   <li>
                    <p>
                     当k=solid，k和ice近似，这时ratio&gt;&gt;1；==&gt; solid与ice有关
                    </p>
                   </li>
                   <li>
                    <p>
                     当k=gas，k和steam接近时，ratio&lt;&lt;1；==&gt; gas与steam有关
                    </p>
                   </li>
                   <li>
                    <p>
                     当k=water/fashion等与2个词都不相关时，ratio≈1。==&gt; 之间无关
                    </p>
                   </li>
                  </ul>
                 </dd>
                </dl>
               </li>
              </ul>
             </dd>
            </dl>
           </li>
          </ol>
          <p>
           <strong>
            通俗的讲下这个比值：1为阈值，当远大于1或远小于1就说明词之间有相关度的；当接近于1时证明无关
           </strong>
          </p>
          <ol class="arabic" start="3">
           <li>
            <p>
             如何表示共现矩阵的概率的比值呢？
            </p>
            <blockquote>
             <div>
              <ul class="simple">
               <li>
                <p>
                 Log-bilinear model：
                 <span class="math notranslate nohighlight">
                  \(w_i \cdot w_j = log P(i|j)\)
                 </span>
                </p>
               </li>
               <li>
                <p>
                 使用向量差值表示：
                 <span class="math notranslate nohighlight">
                  \(w_x \cdot(w_a - w_b) = log \frac{P(x|a)}{P(x|b)}\)
                 </span>
                </p>
               </li>
              </ul>
             </div>
            </blockquote>
           </li>
           <li>
            <dl>
             <dt>
              Glove目标函数为：
             </dt>
             <dd>
              <div class="math notranslate nohighlight">
               \[J=\sum_{i, j=1}^{V} f\left(X_{i j}\right)\left(w_{i}^{T} \tilde{w}_{j}+b_{i}+\tilde{b}_{j}-\log X_{i j}\right)^{2}\]
              </div>
              <ul>
               <li>
                <p>
                 其中f(x)如下图所示：
                </p>
                <blockquote>
                 <div>
                  <img alt="../../_images/glove_fun.png" src="../../_images/glove_fun.png"/>
                 </div>
                </blockquote>
               </li>
               <li>
                <p>
                 <span class="math notranslate nohighlight">
                  \(X_{i j}\)
                 </span>
                 表示词j在词i的上下文中出现的次数。
                </p>
               </li>
              </ul>
             </dd>
            </dl>
           </li>
           <li>
            <dl class="simple">
             <dt>
              Glove的优势
             </dt>
             <dd>
              <ul class="simple">
               <li>
                <p>
                 训练速度快
                </p>
               </li>
               <li>
                <p>
                 可扩展到大语料库
                </p>
               </li>
               <li>
                <p>
                 即时小预料库，其效果也不错
                </p>
               </li>
              </ul>
             </dd>
            </dl>
           </li>
          </ol>
         </section>
         <section id="id29">
          <h3>
           <span class="section-number">
            5.3.4.
           </span>
           怎样评估词向量？
           <a class="headerlink" href="#id29" title="Link to this heading">
            ¶
           </a>
          </h3>
          <ol class="arabic">
           <li>
            <p>
             问题：如何评估NLP的模型。
            </p>
           </li>
           <li>
            <dl class="simple">
             <dt>
              主要从两个方面：内部和外部
             </dt>
             <dd>
              <ul class="simple">
               <li>
                <dl class="simple">
                 <dt>
                  内部（自身评估）
                 </dt>
                 <dd>
                  <ul>
                   <li>
                    <p>
                     评估特定或中间子过程：速度快、有利于理解整个模型
                    </p>
                   </li>
                  </ul>
                 </dd>
                </dl>
               </li>
               <li>
                <dl class="simple">
                 <dt>
                  外部（将词向量应用到下游任务，如推荐、搜索、对话等系统中）
                 </dt>
                 <dd>
                  <ul>
                   <li>
                    <p>
                     在真实场景下进行评估：消耗时间可能过长、不明确子系统是否有交互问题。
                    </p>
                   </li>
                   <li>
                    <p>
                     如果用一个子系统替换另外一个子系统后能提高准确率，那这个模型就很棒了
                    </p>
                   </li>
                   <li>
                    <p>
                     词向量应用到搜索、问答等领域来进行效果评估
                    </p>
                   </li>
                  </ul>
                 </dd>
                </dl>
               </li>
              </ul>
             </dd>
            </dl>
           </li>
           <li>
            <dl>
             <dt>
              评估：词向量
             </dt>
             <dd>
              <ul class="simple">
               <li>
                <p>
                 通过计算余弦距离后并求和来获取语义和相似的句法。
                </p>
               </li>
               <li>
                <p>
                 技巧：丢弃输入的几个关键词，以此来验证词向量的相似度计算性能
                </p>
               </li>
               <li>
                <dl class="simple">
                 <dt>
                  例：a:b :: c:?  man:woman :: king:? man--&gt;king那么woman--&gt;?
                 </dt>
                 <dd>
                  <ul>
                   <li>
                    <p>
                     在语料中找到一点
                     <span class="math notranslate nohighlight">
                      \(x_i\)
                     </span>
                     ，即为和woman最为相近的词
                    </p>
                   </li>
                  </ul>
                 </dd>
                </dl>
               </li>
              </ul>
              <div class="math notranslate nohighlight">
               \[d=\arg \max _{i} \frac{\left(x_{b}-x_{a}+x_{c}\right)^{T} x_{i}}{\left\|x_{b}-x_{a}+x_{c}\right\|}\]
              </div>
              <img alt="../../_images/w2v_eva.png" src="../../_images/w2v_eva.png"/>
              <ul>
               <li>
                <p>
                 下面是glove的某些词向量相似度可视化的结果
                </p>
                <blockquote>
                 <div>
                  <img alt="../../_images/glove_vi.png" src="../../_images/glove_vi.png"/>
                 </div>
                </blockquote>
               </li>
              </ul>
             </dd>
            </dl>
           </li>
           <li>
            <dl>
             <dt>
              评估：相似性和参数
             </dt>
             <dd>
              <ul>
               <li>
                <p>
                 Glove的语义或句法相似度表现的更好，如下表：
                </p>
                <blockquote>
                 <div>
                  <img alt="../../_images/glove_w2v.png" src="../../_images/glove_w2v.png"/>
                 </div>
                </blockquote>
               </li>
               <li>
                <p>
                 特点：语料的规模要大、词向量维数为300更合适
                </p>
                <blockquote>
                 <div>
                  <img alt="../../_images/glove_dim.png" src="../../_images/glove_dim.png"/>
                 </div>
                </blockquote>
               </li>
              </ul>
             </dd>
            </dl>
           </li>
           <li>
            <p>
             相关性评估（距离）
            </p>
            <blockquote>
             <div>
              <img alt="../../_images/glove_corr_eva.png" src="../../_images/glove_corr_eva.png"/>
             </div>
            </blockquote>
           </li>
           <li>
            <dl>
             <dt>
              外部评估
             </dt>
             <dd>
              <ul>
               <li>
                <p>
                 词向量好不好最直接的方法就是应用到实际场景，比如最常用的NER（命名实体识别）任务中
                </p>
                <blockquote>
                 <div>
                  <img alt="../../_images/glove_ner.png" src="../../_images/glove_ner.png"/>
                 </div>
                </blockquote>
               </li>
               <li>
                <p>
                 glove对于NER任务表现理论上还说的过去，但凭上表中的准确率在工业领域中还是很难拿来用的。那么还有什么好的模型或者思路呢？
                </p>
               </li>
               <li>
                <p>
                 从下一小节就开始尝试将词向量输入到神经网络中，来进一步提升下游任务的性能。
                </p>
               </li>
              </ul>
             </dd>
            </dl>
           </li>
          </ol>
         </section>
         <section id="id30">
          <h3>
           <span class="section-number">
            5.3.5.
           </span>
           语义及其歧义性
           <a class="headerlink" href="#id30" title="Link to this heading">
            ¶
           </a>
          </h3>
          <div class="admonition note">
           <p class="admonition-title">
            备注
           </p>
           <p>
            实际上，许多词都是一词多义的，特别是咱们的汉语，更甚有如今曾层出不穷的网络流行语，有时你不懂点八卦还真的猜不来词要表达的真实含义！这就是NLP绕不开的一个问题：歧义性，对应的任务就是：消歧。
           </p>
          </div>
          <ul>
           <li>
            <p>
             看下单词
             <strong>
              pike
             </strong>
             的含义
            </p>
            <blockquote>
             <div>
              <img alt="../../_images/w2v_ambiguity.png" src="../../_images/w2v_ambiguity.png"/>
              <ul class="simple">
               <li>
                <p>
                 看上图易知词的含义还是相当丰富的，如何相对准确的捕捉到当前场景下（上下文）的真实含义就是值得思考和研究的一个问题。
                </p>
               </li>
              </ul>
             </div>
            </blockquote>
           </li>
          </ul>
          <ol class="arabic">
           <li>
            <p>
             论文1：Improving Word Representations Via Global Context And Multiple Word Prototypes (Huang et al. 2012)
            </p>
            <blockquote>
             <div>
              <img alt="../../_images/cluster_word.png" src="../../_images/cluster_word.png"/>
              <ul class="simple">
               <li>
                <p>
                 使用了聚类的思路：通过一些关键词来聚类，但常常出现重叠（误分）的现象
                </p>
               </li>
              </ul>
             </div>
            </blockquote>
           </li>
          </ol>
          <p>
           2. 论文2：Linear Algebraic Structure of Word Senses, with Applications to Polysemy
将同一词的不同语义进行线性叠加
          </p>
          <blockquote>
           <div>
            <div class="math notranslate nohighlight">
             \[ \begin{align}\begin{aligned}\begin{split}v_{\text { pike }}=\alpha_{1} v_{\text { pike }_{1}}+\alpha_{2} v_{\text { pike }_{2}}+\alpha_{3} v_{\text { pike }_{3}} \\ \alpha_{1}=\frac{f_{1}}{f_{1}+f_{2}+f_{3}}\end{split}\\f为词出现的频率\end{aligned}\end{align} \]
            </div>
            <ul class="simple">
             <li>
              <p>
               论文的思路来自词向量的稀疏编码，
              </p>
             </li>
            </ul>
           </div>
          </blockquote>
         </section>
         <section id="classification">
          <h3>
           <span class="section-number">
            5.3.6.
           </span>
           分类（Classification）模型知识点回顾
           <a class="headerlink" href="#classification" title="Link to this heading">
            ¶
           </a>
          </h3>
          <p>
           1. 样本（数据集）：
           <span class="math notranslate nohighlight">
            \(\{x^{(i)},y^{(i)}\}_{1}^{N}\)
           </span>
           ，其中x_i为输入（词、句、文档等）y_i就是标签，
预测的分类目标（正负向、文档主题等）`
          </p>
          <ol class="arabic" start="2">
           <li>
            <p>
             例如简单的二分类
            </p>
            <blockquote>
             <div>
              <ul class="simple">
               <li>
                <p>
                 可视化
                 <a class="reference external" href="https://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html">
                  地址
                 </a>
                </p>
               </li>
              </ul>
              <img alt="../../_images/cls_ex.png" src="../../_images/cls_ex.png"/>
             </div>
            </blockquote>
           </li>
           <li>
            <p>
             一般的机器学习方法或统计方法：假定
             <span class="math notranslate nohighlight">
              \({x_i}\)
             </span>
             为固定大小，我们使用sofmax或逻辑回归算法训练权重参数W，以此来寻找一个决策边界。
+ 例如softmax算法，对于固定的
             <span class="math notranslate nohighlight">
              \({x}\)
             </span>
             预测y：
            </p>
            <blockquote>
             <div>
              <div class="math notranslate nohighlight">
               \[p(y\mid x)=\frac{exp(W_{j.}x)}{\sum_{c=1}^{C}exp(W_{c.}x)}\]
              </div>
              <img alt="../../_images/softmax_detail.png" src="../../_images/softmax_detail.png"/>
              <ul class="simple">
               <li>
                <p>
                 我们的目标就是最大化正确类别y的概率，不过我们一般为了降低运算的复杂度，会转为下式进行计算：
                </p>
               </li>
              </ul>
              <div class="math notranslate nohighlight">
               \[-log P(y|x) = -log(\frac{exp(f_y)}{\sum_{c=1}^{C}exp(f_c)})\]
              </div>
             </div>
            </blockquote>
           </li>
           <li>
            <dl>
             <dt>
              交叉熵损失
             </dt>
             <dd>
              <ol class="arabic">
               <li>
                <p>
                 交叉熵的概念源自信息论中的知识，对于样本实际的概率分布P与模型产生的结果概率分布q，其交叉熵可表示为下式：
                </p>
                <blockquote>
                 <div>
                  <div class="math notranslate nohighlight">
                   \[H(p,q) = -\sum_{c=1}^{C}p(c)logq(c)\]
                  </div>
                 </div>
                </blockquote>
               </li>
               <li>
                <dl>
                 <dt>
                  整个数据集
                  <span class="math notranslate nohighlight">
                   \(\{x^{(i)},y^{(i)}\}_{1}^{N}\)
                  </span>
                  的交叉熵可表示为：
                 </dt>
                 <dd>
                  <div class="math notranslate nohighlight">
                   \[J(\theta) = \frac{1}{N}\sum_{i=1}^{N} - log\bigg(\frac{e^{f_{y_i}}}{\sum_{c=1}^{C}e^{f_c}}\bigg)\]
                  </div>
                  <p>
                   <span class="math notranslate nohighlight">
                    \(f_y = f_y(x)=W_y·x=\sum_{j=1}{d}W_{y_j}·x_j\)
                   </span>
                  </p>
                 </dd>
                </dl>
               </li>
              </ol>
             </dd>
            </dl>
           </li>
           <li>
            <p>
             优化
            </p>
           </li>
          </ol>
          <ul class="simple">
           <li>
            <p>
             传统机器学习方法优化
            </p>
           </li>
          </ul>
          <p>
           对于
           <span class="math notranslate nohighlight">
            \(\theta\)
           </span>
           的数量一般和权重W的维数一致，线性决策模型至少需要一个d维的词向量输入和生成一个
C个类别的分布。因此更新模型的权值，我们需要 C⋅d个参数
          </p>
          <div class="math notranslate nohighlight">
           \[\begin{split}\theta =  \begin{bmatrix}{W_{.1}} \\ \vdots \\ {W_{.d}}  \end{bmatrix} = W(::)\in\mathbb{R}^{Cd}\end{split}\]
          </div>
          <dl>
           <dt>
            梯度优化：更新决策边界的参数
           </dt>
           <dd>
            <div class="math notranslate nohighlight">
             \[\begin{split}\nabla_{\theta}J(\theta) = {\begin{bmatrix} \nabla_{W_{.1}} \\ \vdots \\ \nabla_{W_{.d}}  \end{bmatrix}}\in\mathbb{R}^{Cd}\end{split}\]
            </div>
           </dd>
          </dl>
          <p>
           6. 使用神经网络+词向量分类
一般的分类算法其通常解决的是一些线性问题，表达能力有限，对一些非线性的决策边界无法更好的建模，借用神经网络模型可进一步提升模型的能力。
          </p>
          <blockquote>
           <div>
            <ul>
             <li>
              <dl>
               <dt>
                基于神经网络分类(模型的预测能力更强)
               </dt>
               <dd>
                <img alt="../../_images/neural_cls.jpg" src="../../_images/neural_cls.jpg"/>
               </dd>
              </dl>
             </li>
             <li>
              <dl>
               <dt>
                词向量与神经网络模型结合
               </dt>
               <dd>
                <ul class="simple">
                 <li>
                  <p>
                   同时学习权重W和词向量x，参数量为：cd+vd，非常大的参数量，C表示分类大数量，d表示每个词向量维数，V表示词汇表的大小。
                  </p>
                 </li>
                </ul>
                <div class="math notranslate nohighlight">
                 \[\begin{split}\begin{eqnarray} \nabla_{\theta}J(\theta) = \begin{bmatrix}\nabla_{W_{.1}} \\ \vdots \\ \nabla_{W_{.d}} \\ \nabla_{aardvark} \\ \vdots \\ \nabla_{zebra} \end{bmatrix} \nonumber \end{eqnarray}\\\end{split}\]
                </div>
                <ul class="simple">
                 <li>
                  <p>
                   参数量大意味着表达（拟合数据）能力很强（这里可以联系数学中的多项式，参数多了意味着多项式越长，图像也就越复杂），但过犹不及，容易造成过拟合，模型泛化能力不足，怎么办呢？常见做法就是加入
                   <strong>
                    正则项
                   </strong>
                  </p>
                 </li>
                </ul>
               </dd>
              </dl>
             </li>
            </ul>
           </div>
          </blockquote>
         </section>
         <section id="id32">
          <h3>
           <span class="section-number">
            5.3.7.
           </span>
           小结
           <a class="headerlink" href="#id32" title="Link to this heading">
            ¶
           </a>
          </h3>
          <p>
           本节首先讲了词向量的训练基本过程和常用方法：解决维数高、复杂度高的问题；其次，讲了Golve的主要思想；最后通过分类引出神经网络的优势：非线性能力，能够更好地获取语义信息。后续小节将开始神经网络的篇章。
          </p>
         </section>
        </section>
        <section id="id33">
         <h2>
          <span class="section-number">
           5.4.
          </span>
          神经网络
          <a class="headerlink" href="#id33" title="Link to this heading">
           ¶
          </a>
         </h2>
         <img alt="../../_images/preface-3.png" src="../../_images/preface-3.png"/>
         <div class="admonition note">
          <p class="admonition-title">
           备注
          </p>
          <p>
           本小节主要讲神经网络的基础知识和NLP中的命名实体识别（NER）任务。
          </p>
         </div>
         <section id="neural-network">
          <h3>
           <span class="section-number">
            5.4.1.
           </span>
           Neural NetWork基础
           <a class="headerlink" href="#neural-network" title="Link to this heading">
            ¶
           </a>
          </h3>
          <ol class="arabic">
           <li>
            <p>
             神经网络是指由很多人工神经元构成的网络结构模型，这些人工神经元之间的连接强度是可学习的参数。
            </p>
           </li>
           <li>
            <p>
             人工神经网络（Artificial Neural Network，ANN）是一种模拟人脑神经网络而设计的数据模型或计算模型，它从结构、实现机理和功能上模拟人脑神经网络。
            </p>
           </li>
           <li>
            <dl>
             <dt>
              神经元（Neuron），是构成神经网络的基本单元，其主要是模拟生物神经元的结构和特性，接受一组输入信号并产出输出。
             </dt>
             <dd>
              <ul class="simple">
               <li>
                <p>
                 例如神经元可以是一个二元的逻辑回归单元，典型的结构如下图：
                </p>
               </li>
              </ul>
              <div class="math notranslate nohighlight">
               \[h_{w,b}(x)=f(w^Tx+b)\]
              </div>
              <img alt="../../_images/neuron.png" src="../../_images/neuron.png"/>
              <p>
               其中，
               <span class="math notranslate nohighlight">
                \(f(z) = \frac{1}{1+e^{-z}}\)
               </span>
               。f称为
               <strong>
                激活函数
               </strong>
               例如sigmoid函数：Logistic、Tanh；w称为
               <strong>
                权重
               </strong>
               ，表示信号的强弱（特征的重要程度）；b称为
               <strong>
                偏置
               </strong>
               ；h称为
               <strong>
                隐藏层
               </strong>
               ；x表示
               <strong>
                输入的特征值
               </strong>
               。
在本例的逻辑回归模型中，w,b就是这个神经元的参数。
              </p>
              <img alt="../../_images/activate_fun.png" src="../../_images/activate_fun.png"/>
             </dd>
            </dl>
           </li>
          </ol>
          <p>
           4. 神经网络结构
还是上面的神经元，但是是同时运行多个逻辑回归单元。比如有一下几种形式：
          </p>
          <blockquote>
           <div>
            <img alt="../../_images/NN_1.png" src="../../_images/NN_1.png"/>
            <img alt="../../_images/NN_2.png" src="../../_images/NN_2.png"/>
            <img alt="../../_images/NN_3.png" src="../../_images/NN_3.png"/>
            <ul class="simple">
             <li>
              <p>
               每一次神经元用矩阵符号表示
              </p>
             </li>
            </ul>
            <img alt="../../_images/NN_4.png" src="../../_images/NN_4.png"/>
            <div class="math notranslate nohighlight">
             \[ \begin{align}\begin{aligned}a_1 = f(W_{11} x_{1}+W_{12} x_{2}+W_{13} x_{3}+b_{1}\\a_2 = f(W_{21} x_{1}+W_{22} x_{2}+W_{23} x_{3}+b_{2}\\...\end{aligned}\end{align} \]
            </div>
            <dl>
             <dt>
              可用矩阵符号表示：
             </dt>
             <dd>
              <div class="math notranslate nohighlight">
               \[ \begin{align}\begin{aligned}z = WX + b\\a = f(z)\end{aligned}\end{align} \]
              </div>
              <p>
               激活函数f(x)逐元素进行相乘。
               <span class="math notranslate nohighlight">
                \(f([z_1,z_2,z_3]) = [f(z_1),f(z_2),f(z_3)]\)
               </span>
              </p>
             </dd>
            </dl>
           </div>
          </blockquote>
          <ol class="arabic" start="5">
           <li>
            <dl>
             <dt>
              为什么需要非线性f激活函数
             </dt>
             <dd>
              <ul class="simple">
               <li>
                <p>
                 一句话总结：提高模型的表示能力或者学习能力。为什么呢？
                </p>
               </li>
               <li>
                <p>
                 因为没有非线性变化，我们就无需选择神经网络模型了，因为只能进行线性变换和传统的机器学习模型类似了
                </p>
               </li>
               <li>
                <p>
                 因为线性变换组合后还是线性变换，不能进行深层次的特征学习
                </p>
               </li>
               <li>
                <p>
                 非线性变换的层数（隐藏层）越多，那么模型的就能拟合更为复杂的数据（联想下多项式函数，一次、二次、三次甚至更高次函数拟合样本点程度是完全不同的，当然复杂度也是递增的），不过也很容易造成过拟合，这个度要想拿捏的好，是一门玄学（运气+实力）
                </p>
               </li>
               <li>
                <p>
                 这里补充点关于激活函数的知识：
                </p>
               </li>
              </ul>
              <div class="admonition note">
               <p class="admonition-title">
                备注
               </p>
               <ul class="simple">
                <li>
                 <dl class="simple">
                  <dt>
                   激活函数：
                  </dt>
                  <dd>
                   <ul>
                    <li>
                     <p>
                      连续并可导（允许少数点上不可导）的非线性函数。可导的激活函数可以直接利用数值优化的方法来学习网络参数。
                     </p>
                    </li>
                    <li>
                     <p>
                      激活函数及其导函数要尽可能的简单，有利于提高网络计算效率。
                     </p>
                    </li>
                    <li>
                     <p>
                      激活函数的导函数的值域要在一个合适的区间内，不能太大也不能太小，否则会影响训练的效率和稳定性。
                     </p>
                    </li>
                   </ul>
                  </dd>
                 </dl>
                </li>
                <li>
                 <dl class="simple">
                  <dt>
                   常见激活函数
                  </dt>
                  <dd>
                   <ul>
                    <li>
                     <p>
                      Sigmoid型激活函数：是指一类S型曲线函数，为两端饱和函数。常用的Sigmoid型函数有Logistic函数和Tanh函数。
                     </p>
                    </li>
                    <li>
                     <p>
                      修正线性单元（Rectified Linear Unit，ReLU）。常用的激活函数之一，图像类似一个斜坡。
                     </p>
                    </li>
                    <li>
                     <p>
                      指数线性单元（Exponential Linear Unit，ELU）。是一个近似的零中心化的非线性函数。
                     </p>
                    </li>
                   </ul>
                  </dd>
                 </dl>
                </li>
               </ul>
              </div>
             </dd>
            </dl>
           </li>
          </ol>
         </section>
         <section id="id34">
          <h3>
           <span class="section-number">
            5.4.2.
           </span>
           命名实体识别介绍
           <a class="headerlink" href="#id34" title="Link to this heading">
            ¶
           </a>
          </h3>
          <div class="admonition note">
           <p class="admonition-title">
            备注
           </p>
           <p>
            命名实体识别（Named Entity Recognition，NER）。NLP领域的一个基础型子任务。
           </p>
          </div>
          <ol class="arabic">
           <li>
            <dl class="simple">
             <dt>
              一些用途
             </dt>
             <dd>
              <ul class="simple">
               <li>
                <p>
                 获取文档中的实体名称。
                </p>
               </li>
               <li>
                <p>
                 问答、对话等系统需要实体。
                </p>
               </li>
               <li>
                <p>
                 从实体与实体之间的关联中获取信息。
                </p>
               </li>
               <li>
                <p>
                 也可扩展到填槽（slot-filling）分类的任务中。在对话系统中
                 <strong>
                  填槽
                 </strong>
                 指的是为了让用户意图转化为用户明确的指令而补全信息的过程。
                </p>
               </li>
               <li>
                <p>
                 在构建知识库/知识图谱的过程中，获取命名实体也是重要的一环。
                </p>
               </li>
              </ul>
             </dd>
            </dl>
           </li>
           <li>
            <dl>
             <dt>
              思路
             </dt>
             <dd>
              <ul class="simple">
               <li>
                <dl class="simple">
                 <dt>
                  首先通过上下词对单词进行
                  <strong>
                   分类
                  </strong>
                  ，然后将实体提取为词
                  <strong>
                   序列
                  </strong>
                  来预测实体。
                 </dt>
                 <dd>
                  <ul>
                   <li>
                    <p>
                     我们可发现NER不仅是分类问题，还是一个序列问题，也称为序列标注问题
                    </p>
                   </li>
                  </ul>
                 </dd>
                </dl>
               </li>
               <li>
                <p>
                 BIO标注体系：B-实体起始位置、I-实体结束位置、O-非实体
                </p>
               </li>
              </ul>
              <img alt="../../_images/NER.png" src="../../_images/NER.png"/>
              <ul class="simple">
               <li>
                <p>
                 补充一点NER的知识：
                </p>
               </li>
              </ul>
              <div class="admonition note">
               <p class="admonition-title">
                备注
               </p>
               <ul class="simple">
                <li>
                 <p>
                  NER的目的是从一段文本中找出实体同时也需要标注出实体的位置，实体一般包含人名、地名、组织名等。
                 </p>
                </li>
                <li>
                 <p>
                  NER标注是使用的标签体系包括：IO、BIO（常用）、BMEWO、BMEWO+，一般地，标签体系越复杂其标注结果也更准确。
                 </p>
                </li>
                <li>
                 <p>
                  NER常用算法：BiLSTM+CRF、BiLSTM-LAN、也可结合词典进行实体识别，具备扩展性
                 </p>
                </li>
                <li>
                 <p>
                  标注工具：
                  <a class="reference external" href="http://brat.nlplab.org/examples.html">
                   brat
                  </a>
                 </p>
                </li>
               </ul>
              </div>
             </dd>
            </dl>
           </li>
           <li>
            <dl class="simple">
             <dt>
              NER的难点何在？
             </dt>
             <dd>
              <ul class="simple">
               <li>
                <p>
                 边界问题。NER任务在文本的处理中是很常用的一个工具，但其本身会对一些文本中的实体边界识别有偏差。这一点在我的实际业务中也遇到过：“股东**李杨楠**近日……”，对人名会识别为：“杨楠”，而实际是：“李杨楠”，我的处理策略之一就是加一个人名词典。
                </p>
               </li>
               <li>
                <p>
                 实体似是而非问题。“Future School"是一个学校（ORG）名称，还是其本意未来的学校。
                </p>
               </li>
               <li>
                <p>
                 不确定是人还是机构或地点。
                </p>
               </li>
               <li>
                <p>
                 实体识别依赖上下文。我遇到的实际业务问题：公司xxx先生，由于公司干扰会将PER：xxx识别为ORG。
                </p>
               </li>
              </ul>
             </dd>
            </dl>
           </li>
          </ol>
          <p>
           <strong>
            个人结合实际对于上述难点问题有两个解决思路：一是模型本身的调整（语料、结合实体上下文进行分类、引入知识等）；二是维护一个词典，将识别有误的实体添加到字典，这个方法感觉比较常用。
           </strong>
          </p>
          <ol class="arabic" start="4">
           <li>
            <dl>
             <dt>
              尝试：词基于窗口（上下文）分类
             </dt>
             <dd>
              <ul>
               <li>
                <p>
                 思路：classify a word in its context window of neighboring words.
                </p>
               </li>
               <li>
                <p>
                 例如：在实体的上下文中判断是人名、地名、机构名或者什么也不是。
                </p>
               </li>
               <li>
                <dl>
                 <dt>
                  实现策略
                 </dt>
                 <dd>
                  <ul>
                   <li>
                    <dl class="simple">
                     <dt>
                      之一：每个词的上下文存在差别，所以可对上下文窗口的词向量进行平均化，然后再进行单词分类。
                     </dt>
                     <dd>
                      <ul class="simple">
                       <li>
                        <p>
                         缺点：丢失位置信息。（为什么呢？我想是word embedding被运算的造成的）
                        </p>
                       </li>
                      </ul>
                     </dd>
                    </dl>
                   </li>
                   <li>
                    <dl>
                     <dt>
                      之二：Softmax
                     </dt>
                     <dd>
                      <ul class="simple">
                       <li>
                        <p>
                         训练一个softmax分类器对实体以及其窗口词（上下文）整体进行分类
                        </p>
                       </li>
                      </ul>
                      <img alt="../../_images/ner_base_windows.png" src="../../_images/ner_base_windows.png"/>
                      <ul>
                       <li>
                        <dl>
                         <dt>
                          数学含义
                         </dt>
                         <dd>
                          <p>
                           softmax分类器：
                          </p>
                          <div class="math notranslate nohighlight">
                           \[\widehat{y}_y = p(y|x) = {exp(W_y \cdot x) \over \sum ^C_{c=1}exp(W_c \cdot x)}\]
                          </div>
                          <p>
                           交叉熵损失函数:
                          </p>
                          <div class="math notranslate nohighlight">
                           \[J(\theta) = {1\over N} \sum^N_{i=1} -log({e^{f_{yi}}\over \sum^C_{c=1}e^{f_c}})\]
                          </div>
                         </dd>
                        </dl>
                       </li>
                      </ul>
                     </dd>
                    </dl>
                   </li>
                   <li>
                    <dl>
                     <dt>
                      进阶之三：多层感知机
                     </dt>
                     <dd>
                      <ul>
                       <li>
                        <p>
                         参考论文
                         <a class="reference external" href="http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf">
                          (2011)Natural Language Processing (Almost) from Scratch
                         </a>
                        </p>
                       </li>
                       <li>
                        <p>
                         思路：在softmax分类器加入中间层（引入非线性）提升分类器的分类能力（复杂性）。根据是否是需要分类的实体进行权重的分配。
                        </p>
                       </li>
                       <li>
                        <p>
                         神经网络前向计算
                        </p>
                        <blockquote>
                         <div>
                          <img alt="../../_images/ner_softmax_nerul.png" src="../../_images/ner_softmax_nerul.png"/>
                         </div>
                        </blockquote>
                       </li>
                       <li>
                        <p>
                         中间层可与输入的词向量进行非线性的变换（赋予不同的权重）
                        </p>
                       </li>
                       <li>
                        <dl class="simple">
                         <dt>
                          Objective Function 我们可使用Hinge损失函数（Max-margin loss）。目的是使目标窗口得分更高，其他窗口得分降低
                         </dt>
                         <dd>
                          <ul class="simple">
                           <li>
                            <p>
                             <span class="math notranslate nohighlight">
                              \(s = score("museums  \ in \ Paris \ are \ amazing”)\)
                             </span>
                            </p>
                           </li>
                           <li>
                            <p>
                             <span class="math notranslate nohighlight">
                              \(s_c = score("Not \ all \ museums  \ in \ Paris)\)
                             </span>
                            </p>
                           </li>
                           <li>
                            <p>
                             <span class="math notranslate nohighlight">
                              \(Minimize(J) =\max(0,1-s+s_{c})\)
                             </span>
                            </p>
                           </li>
                           <li>
                            <p>
                             函数为不连续可微，因此可计算梯度。
                            </p>
                           </li>
                          </ul>
                         </dd>
                        </dl>
                       </li>
                      </ul>
                     </dd>
                    </dl>
                   </li>
                  </ul>
                  <div class="admonition note">
                   <p class="admonition-title">
                    备注
                   </p>
                   <p>
                    Hinge损失函数(又称，max-margin objective)对于两类分类问题，假设y 和f(x, θ)的取值为{−1, +1}。Hinge
损失函数（Hinge Loss Function）为:
                   </p>
                   <div class="math notranslate nohighlight">
                    \[L(y, f(x, \theta))=\max(0, 1-yf(x, \theta))\]
                   </div>
                  </div>
                 </dd>
                </dl>
               </li>
              </ul>
             </dd>
            </dl>
           </li>
          </ol>
          <p>
           小练习: 理解并使用Python实现Softmax分类算法。
          </p>
         </section>
        </section>
        <section id="bp">
         <h2>
          <span class="section-number">
           5.5.
          </span>
          矩阵计算与BP（反向传播）算法
          <a class="headerlink" href="#bp" title="Link to this heading">
           ¶
          </a>
         </h2>
         <section id="id35">
          <h3>
           <span class="section-number">
            5.5.1.
           </span>
           矩阵的梯度计算
           <a class="headerlink" href="#id35" title="Link to this heading">
            ¶
           </a>
          </h3>
          <ul class="simple">
           <li>
            <dl class="simple">
             <dt>
              都是基础的高数计算，可直接查看
              <a class="reference external" href="http://web.stanford.edu/class/cs224n/slides/cs224n-2020-lecture04-neuralnets.pdf">
               幻灯
              </a>
             </dt>
             <dd>
              <ul>
               <li>
                <p>
                 求导链式法则
                </p>
               </li>
               <li>
                <p>
                 复合函数求导
                </p>
               </li>
               <li>
                <p>
                 Jacobian matrix（Jacobian: Vector in, Vector out）
                </p>
               </li>
               <li>
                <p>
                 推荐补充一份关于微分及BP的
                 <a class="reference external" href="http://cs231n.stanford.edu/handouts/derivatives.pdf">
                  文稿
                 </a>
                </p>
               </li>
              </ul>
             </dd>
            </dl>
           </li>
          </ul>
         </section>
         <section id="id38">
          <h3>
           <span class="section-number">
            5.5.2.
           </span>
           BP算法（重点）
           <a class="headerlink" href="#id38" title="Link to this heading">
            ¶
           </a>
          </h3>
          <p>
           甜点
           <a class="reference external" href="https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b">
            Yes you should understand backprop
           </a>
          </p>
          <div class="admonition note">
           <p class="admonition-title">
            备注
           </p>
           <ul class="simple">
            <li>
             <p>
              进行微分并使用链式法则。共享参数以减少计算量。
             </p>
            </li>
           </ul>
          </div>
          <ol class="arabic simple">
           <li>
            <p>
             计算图和BP
            </p>
           </li>
           <li>
            <p>
             BP算法核心知识
            </p>
           </li>
           <li>
            <p>
             计算效率
            </p>
           </li>
          </ol>
         </section>
         <section id="id39">
          <h3>
           <span class="section-number">
            5.5.3.
           </span>
           总结
           <a class="headerlink" href="#id39" title="Link to this heading">
            ¶
           </a>
          </h3>
         </section>
        </section>
       </section>
       <div class="section ablog__blog_comments">
       </div>
      </article>
     </div>
    </div>
   </main>
  </div>
  <footer class="md-footer">
   <div class="md-footer-nav">
    <nav class="md-footer-nav__inner md-grid">
     <a class="md-flex md-footer-nav__link md-footer-nav__link--prev" href="dl.html" rel="prev" title="3. 深度学习">
      <div class="md-flex__cell md-flex__cell--shrink">
       <i class="md-icon md-icon--arrow-back md-footer-nav__button">
       </i>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
       <span class="md-flex__ellipsis">
        <span class="md-footer-nav__direction">
         "Previous"
        </span>
        <span class="section-number">
         3.
        </span>
        深度学习
       </span>
      </div>
     </a>
     <a class="md-flex md-footer-nav__link md-footer-nav__link--next" href="kg.html" rel="next" title="6. 知识图谱(Knowledge Graph)">
      <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
       <span class="md-flex__ellipsis">
        <span class="md-footer-nav__direction">
         "Next"
        </span>
        <span class="section-number">
         6.
        </span>
        知识图谱(Knowledge Graph)
       </span>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
       <i class="md-icon md-icon--arrow-forward md-footer-nav__button">
       </i>
      </div>
     </a>
    </nav>
   </div>
   <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
     <div class="md-footer-copyright">
      <div class="md-footer-copyright__highlight">
       © Copyright 2020-2025, LiHangHang.
      </div>
      Created using
      <a href="http://www.sphinx-doc.org/">
       Sphinx
      </a>
      8.2.3.
             and
      <a href="https://github.com/bashtage/sphinx-material/">
       Material for
              Sphinx
      </a>
     </div>
    </div>
   </div>
  </footer>
  <script src="../../_static/javascripts/application.js">
  </script>
  <script>
   app.initialize({version: "1.0.4", url: {base: ".."}})
  </script>
 </body>
</html>
